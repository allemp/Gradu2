@online{bakerAcceleratingNeuralArchitecture2017,
  title = {Accelerating {{Neural Architecture Search}} Using {{Performance Prediction}}},
  author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  date = {2017-11-08},
  number = {arXiv:1705.10823},
  eprint = {arXiv:1705.10823},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1705.10823},
  url = {http://arxiv.org/abs/1705.10823},
  urldate = {2023-01-25},
  abstract = {Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\master\\Zotero\\storage\\I93LMDHA\\Baker et al_2017_Accelerating Neural Architecture Search using Performance Prediction.pdf;C\:\\Users\\master\\Zotero\\storage\\IWHABXLB\\1705.html}
}

@inproceedings{breckMLTestScore2017a,
  title = {The {{ML Test Score}}: {{A Rubric}} for {{ML Production Readiness}} and {{Technical Debt Reduction}}},
  shorttitle = {The {{ML Test Score}}},
  booktitle = {Proceedings of {{IEEE Big Data}}},
  author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
  date = {2017},
  file = {C\:\\Users\\master\\Zotero\\storage\\LEKCIDWU\\Breck et al_2017_The ML Test Score.pdf}
}

@online{brunnertPerformanceorientedDevOpsResearch2015,
  title = {Performance-Oriented {{DevOps}}: {{A Research Agenda}}},
  shorttitle = {Performance-Oriented {{DevOps}}},
  author = {Brunnert, Andreas and van Hoorn, Andre and Willnecker, Felix and Danciu, Alexandru and Hasselbring, Wilhelm and Heger, Christoph and Herbst, Nikolas and Jamshidi, Pooyan and Jung, Reiner and von Kistowski, Joakim and Koziolek, Anne and Kroß, Johannes and Spinner, Simon and Vögele, Christian and Walter, Jürgen and Wert, Alexander},
  options = {useprefix=true},
  date = {2015-08-18},
  number = {arXiv:1508.04752},
  eprint = {arXiv:1508.04752},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1508.04752},
  url = {http://arxiv.org/abs/1508.04752},
  urldate = {2023-01-17},
  abstract = {DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. Performance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.},
  pubstate = {preprint},
  keywords = {Computer Science - Performance,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\9MDJPVS9\\Brunnert et al_2015_Performance-oriented DevOps.pdf;C\:\\Users\\master\\Zotero\\storage\\3DKEU57U\\1508.html}
}

@inproceedings{cardososilvaBenchmarkingMachineLearning2020,
  title = {Benchmarking {{Machine Learning Solutions}} in {{Production}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Cardoso Silva, Lucas and Rezende Zagatti, Fernando and Silva Sette, Bruno and Nildaimon dos Santos Silva, Lucas and Lucrédio, Daniel and Furtado Silva, Diego and de Medeiros Caseli, Helena},
  options = {useprefix=true},
  date = {2020-12},
  pages = {626--633},
  doi = {10.1109/ICMLA51294.2020.00104},
  abstract = {Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.},
  eventtitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  keywords = {_tablet,Benchmark,Benchmark testing,Machine learning,Machine Learning,MLOps,Monitoring,Production,Systems operation,Task analysis,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\7WJUB6UG\\Cardoso Silva et al_2020_Benchmarking Machine Learning Solutions in Production.pdf;C\:\\Users\\master\\Zotero\\storage\\3PXPYX5A\\9356298.html}
}

@online{coteQualityIssuesMachine2022,
  title = {Quality Issues in {{Machine Learning Software Systems}}},
  author = {Côté, Pierre-Olivier and Nikanjam, Amin and Bouchoucha, Rached and Khomh, Foutse},
  date = {2022-08-22},
  number = {arXiv:2208.08982},
  eprint = {arXiv:2208.08982},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2208.08982},
  url = {http://arxiv.org/abs/2208.08982},
  urldate = {2022-12-07},
  abstract = {Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threat to human life. The quality assurance of MLSSs is considered as a challenging task and currently is a hot research topic. Moreover, it is important to cover all various aspects of the quality in MLSSs. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of bad-practices related to poor quality in MLSSs. Method: We plan to conduct a set of interviews with practitioners/experts, believing that interviews are the best method to retrieve their experience and practices when dealing with quality issues. We expect that the catalog of issues developed at this step will also help us later to identify the severity, root causes, and possible remedy for quality issues of MLSSs, allowing us to develop efficient quality assurance tools for ML models and MLSSs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\NPNEQEVA\\Côté et al_2022_Quality issues in Machine Learning Software Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\ES8GKMEI\\2208.html}
}

@article{dengMNISTDatabaseHandwritten2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Machine learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\YD2EFMVK\\Deng_2012_The MNIST Database of Handwritten Digit Images for Machine Learning Research.pdf;C\:\\Users\\master\\Zotero\\storage\\8XXBPIR7\\6296535.html}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  date = {2012-10-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {55},
  number = {10},
  pages = {78--87},
  issn = {0001-0782},
  doi = {10.1145/2347736.2347755},
  url = {https://doi.org/10.1145/2347736.2347755},
  urldate = {2023-03-14},
  abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
  file = {C\:\\Users\\master\\Zotero\\storage\\J2LSZY59\\Domingos_2012_A few useful things to know about machine learning.pdf}
}

@article{fernandez-lozanoMethodologyDesignExperiments2016,
  title = {A Methodology for the Design of Experiments in Computational Intelligence with Multiple Regression Models},
  author = {Fernandez-Lozano, Carlos and Gestal, Marcos and Munteanu, Cristian R. and Dorado, Julian and Pazos, Alejandro},
  date = {2016-12-01},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {4},
  eprint = {27920952},
  eprinttype = {pmid},
  pages = {e2721},
  issn = {2167-8359},
  doi = {10.7717/peerj.2721},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5136129/},
  urldate = {2023-02-15},
  abstract = {The design of experiments and the validation of the results achieved with them are vital in any research study. This paper focuses on the use of different Machine Learning approaches for regression tasks in the field of Computational Intelligence and especially on a correct comparison between the different results provided for different methods, as those techniques are complex systems that require further study to be fully understood. A methodology commonly accepted in Computational intelligence is implemented in an R package called RRegrs. This package includes ten simple and complex regression models to carry out predictive modeling using Machine Learning and well-known regression algorithms. The framework for experimental design presented herein is evaluated and validated against RRegrs. Our results are different for three out of five state-of-the-art simple datasets and it can be stated that the selection of the best model according to our proposal is statistically significant and relevant. It is of relevance to use a statistical approach to indicate whether the differences are statistically significant using this kind of algorithms. Furthermore, our results with three real complex datasets report different best models than with the previously published methodology. Our final goal is to provide a complete methodology for the use of different steps in order to compare the results obtained in Computational Intelligence problems, as well as from other fields, such as for bioinformatics, cheminformatics, etc., given that our proposal is open and~modifiable.},
  pmcid = {PMC5136129},
  file = {C\:\\Users\\master\\Zotero\\storage\\R4QQLYJX\\Fernandez-Lozano et al_2016_A methodology for the design of experiments in computational intelligence with.pdf}
}

@article{finzerDataScienceEducation2013,
  title = {The {{Data Science Education Dilemma}}},
  author = {Finzer, William},
  date = {2013},
  journaltitle = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {2},
  doi = {10.5070/T572013891},
  url = {https://escholarship.org/uc/item/7gv0q9dc},
  urldate = {2023-01-17},
  abstract = {The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter-disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.},
  langid = {english},
  file = {C\:\\Users\\master\\Zotero\\storage\\YWX2L6LE\\Finzer_2013_The Data Science Education Dilemma.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  number = {arXiv:1503.02531},
  eprint = {arXiv:1503.02531},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-02-03},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\K9JTAWDA\\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\master\\Zotero\\storage\\JENB6RVC\\1503.html}
}

@online{imbreaAutomatedMachineLearning2021,
  title = {Automated {{Machine Learning Techniques}} for {{Data Streams}}},
  author = {Imbrea, Alexandru-Ionut},
  date = {2021-06-14},
  number = {arXiv:2106.07317},
  eprint = {arXiv:2106.07317},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2106.07317},
  url = {http://arxiv.org/abs/2106.07317},
  urldate = {2023-01-25},
  abstract = {Automated machine learning techniques benefited from tremendous research progress in recently. These developments and the continuous-growing demand for machine learning experts led to the development of numerous AutoML tools. However, these tools assume that the entire training dataset is available upfront and that the underlying distribution does not change over time. These assumptions do not hold in a data stream mining setting where an unbounded stream of data cannot be stored and is likely to manifest concept drift. Industry applications of machine learning on streaming data become more popular due to the increasing adoption of real-time streaming patterns in IoT, microservices architectures, web analytics, and other fields. The research summarized in this paper surveys the state-of-the-art open-source AutoML tools, applies them to data collected from streams, and measures how their performance changes over time. For comparative purposes, batch, batch incremental and instance incremental estimators are applied and compared. Moreover, a meta-learning technique for online algorithm selection based on meta-feature extraction is proposed and compared while model replacement and continual AutoML techniques are discussed. The results show that off-the-shelf AutoML tools can provide satisfactory results but in the presence of concept drift, detection or adaptation techniques have to be applied to maintain the predictive accuracy over time.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\PWERNVYQ\\Imbrea_2021_Automated Machine Learning Techniques for Data Streams.pdf;C\:\\Users\\master\\Zotero\\storage\\ER4UAF2R\\2106.html}
}

@online{kreuzbergerMachineLearningOperations2022,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  date = {2022-05-14},
  number = {arXiv:2205.02302},
  eprint = {arXiv:2205.02302},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2205.02302},
  url = {http://arxiv.org/abs/2205.02302},
  urldate = {2022-12-07},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  pubstate = {preprint},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\XUXSDP43\\Kreuzberger et al_2022_Machine Learning Operations (MLOps).pdf;C\:\\Users\\master\\Zotero\\storage\\6LMZFC2P\\2205.html}
}

@online{leBuildingHighlevelFeatures2012,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
  date = {2012-07-12},
  number = {arXiv:1112.6209},
  eprint = {arXiv:1112.6209},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1112.6209},
  url = {http://arxiv.org/abs/1112.6209},
  urldate = {2023-02-03},
  abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8\% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70\% relative improvement over the previous state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\HA2RAJ2C\\Le et al_2012_Building high-level features using large scale unsupervised learning.pdf;C\:\\Users\\master\\Zotero\\storage\\7CYMMR8Y\\1112.html}
}

@online{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2018-07-13},
  number = {arXiv:1807.05118},
  eprint = {arXiv:1807.05118},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1807.05118},
  url = {http://arxiv.org/abs/1807.05118},
  urldate = {2023-02-22},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\CZZSUU97\\Liaw et al_2018_Tune.pdf;C\:\\Users\\master\\Zotero\\storage\\V45V3LPN\\1807.html}
}

@online{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2022-09-06},
  number = {arXiv:2108.02497},
  eprint = {arXiv:2108.02497},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2108.02497},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-12-07},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\278JM9KT\\Lones_2022_How to avoid machine learning pitfalls.pdf;C\:\\Users\\master\\Zotero\\storage\\6RWMHTU8\\2108.html}
}

@online{luoNeuralArchitectureOptimization2019,
  title = {Neural {{Architecture Optimization}}},
  author = {Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  date = {2019-09-04},
  number = {arXiv:1808.07233},
  eprint = {arXiv:1808.07233},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1808.07233},
  url = {http://arxiv.org/abs/1808.07233},
  urldate = {2023-01-25},
  abstract = {Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93\% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93\%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\6K6PGBVJ\\Luo et al_2019_Neural Architecture Optimization.pdf;C\:\\Users\\master\\Zotero\\storage\\KCLHHPPH\\1808.html}
}

@online{maclaurinGradientbasedHyperparameterOptimization2015,
  title = {Gradient-Based {{Hyperparameter Optimization}} through {{Reversible Learning}}},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
  date = {2015-04-02},
  number = {arXiv:1502.03492},
  eprint = {arXiv:1502.03492},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1502.03492},
  url = {http://arxiv.org/abs/1502.03492},
  urldate = {2023-02-03},
  abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\IXIGPQTZ\\Maclaurin et al_2015_Gradient-based Hyperparameter Optimization through Reversible Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\RTHEZPGN\\1502.html}
}

@article{mishraDevOpsSoftwareQuality2020,
  title = {{{DevOps}} and Software Quality: {{A}} Systematic Mapping},
  shorttitle = {{{DevOps}} and Software Quality},
  author = {Mishra, Alok and Otaiwi, Ziadoon},
  date = {2020-11-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {38},
  pages = {100308},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100308},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013720304081},
  urldate = {2023-01-17},
  abstract = {Quality pressure is one of the factors affecting processes for software development in its various stages. DevOps is one of the proposed solutions to such pressure. The primary focus of DevOps is to increase the deployment speed, frequency and quality. DevOps is a mixture of different developments and operations to its multitudinous ramifications in software development industries, DevOps have attracted the interest of many researchers. There are considerable literature surveys on this critical innovation in software development, yet, little attention has been given to DevOps impact on software quality. This research is aimed at analyzing the implications of DevOps features on software quality. DevOps can also be referred to a change in organization cultures aimed at removal of gaps between the development and operations of an organization. The adoption of DevOps in an organization provides many benefits including quality but also brings challenges to an organization. This study presents systematic mapping of the impact of DevOps on software quality. The results of this study provide a better understanding of DevOps on software quality for both professionals and researchers working in this area. The study shows research was mainly focused in automation, culture, continuous delivery, fast feedback of DevOps. There is need of further research in many areas of DevOps (for instance: measurement, development of metrics of different stages to assess its performance, culture, practices toward ensuring quality assurance, and quality factors such as usability, efficiency, software maintainability and portability).},
  langid = {english},
  keywords = {_tablet,Automation,Development,DevOps,Measurement,Operations,Software,Software quality,Systematic mapping},
  file = {C\:\\Users\\master\\Zotero\\storage\\FI2BPQCJ\\Mishra_Otaiwi_2020_DevOps and software quality.pdf;C\:\\Users\\master\\Zotero\\storage\\NTDWUG9S\\S1574013720304081.html}
}

@article{myllyahoMisbehaviourFaultTolerance2022,
  title = {On Misbehaviour and Fault Tolerance in Machine Learning Systems},
  author = {Myllyaho, Lalli and Raatikainen, Mikko and Männistö, Tomi and Nurminen, Jukka K. and Mikkonen, Tommi},
  date = {2022-01-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {183},
  pages = {111096},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111096},
  url = {https://www.sciencedirect.com/science/article/pii/S016412122100193X},
  urldate = {2023-02-03},
  abstract = {Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts.},
  langid = {english},
  keywords = {Case study,Fault tolerance,Machine learning,Software architecture,Software engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\PF8PE5AX\\Myllyaho et al_2022_On misbehaviour and fault tolerance in machine learning systems.pdf;C\:\\Users\\master\\Zotero\\storage\\IRCSGYBT\\S016412122100193X.html}
}

@article{olsonPMLBLargeBenchmark2017,
  title = {{{PMLB}}: A Large Benchmark Suite for Machine Learning Evaluation and Comparison},
  shorttitle = {{{PMLB}}},
  author = {Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
  date = {2017-12-11},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {10},
  number = {1},
  pages = {36},
  issn = {1756-0381},
  doi = {10.1186/s13040-017-0154-4},
  url = {https://doi.org/10.1186/s13040-017-0154-4},
  urldate = {2023-02-22},
  abstract = {The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists.},
  keywords = {Benchmarking,Data repository,Machine learning,Model evaluation},
  file = {C\:\\Users\\master\\Zotero\\storage\\DQMHLZIA\\Olson et al_2017_PMLB.pdf;C\:\\Users\\master\\Zotero\\storage\\T9Y2UVYZ\\s13040-017-0154-4.html}
}

@online{rivolliCharacterizingClassificationDatasets2019,
  title = {Characterizing Classification Datasets: A Study of Meta-Features for Meta-Learning},
  shorttitle = {Characterizing Classification Datasets},
  author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
  options = {useprefix=true},
  date = {2019-08-26},
  number = {arXiv:1808.10406},
  eprint = {arXiv:1808.10406},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1808.10406},
  url = {http://arxiv.org/abs/1808.10406},
  urldate = {2023-01-25},
  abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\RBLAW9V5\\Rivolli et al_2019_Characterizing classification datasets.pdf;C\:\\Users\\master\\Zotero\\storage\\GZWP5W9J\\1808.html}
}

@online{shankarOperationalizingMachineLearning2022,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  date = {2022-09-16},
  number = {arXiv:2209.09125},
  eprint = {arXiv:2209.09125},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.2209.09125},
  url = {http://arxiv.org/abs/2209.09125},
  urldate = {2022-12-07},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  pubstate = {preprint},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\K7KRWSL2\\Shankar et al_2022_Operationalizing Machine Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\AWM9SK8B\\2209.html}
}

@article{shmueliExplainPredict2010a,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  date = {2010-08-01},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {25},
  number = {3},
  eprint = {1101.0891},
  eprinttype = {arxiv},
  eprintclass = {stat},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {http://arxiv.org/abs/1101.0891},
  urldate = {2023-03-14},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\master\\Zotero\\storage\\AAY68PXV\\Shmueli_2010_To Explain or to Predict.pdf;C\:\\Users\\master\\Zotero\\storage\\DT7EAVTS\\1101.html}
}

@article{wallerIncludingPerformanceBenchmarks2015,
  title = {Including {{Performance Benchmarks}} into {{Continuous Integration}} to {{Enable DevOps}}},
  author = {Waller, Jan and Ehmke, Nils C. and Hasselbring, Wilhelm},
  date = {2015-04-03},
  journaltitle = {ACM SIGSOFT Software Engineering Notes},
  shortjournal = {SIGSOFT Softw. Eng. Notes},
  volume = {40},
  number = {2},
  pages = {1--4},
  issn = {0163-5948},
  doi = {10.1145/2735399.2735416},
  url = {https://doi.org/10.1145/2735399.2735416},
  urldate = {2023-01-17},
  abstract = {The DevOps movement intends to improve communication, collaboration, and integration between software developers (Dev) and IT operations professionals (Ops). Automation of software quality assurance is key to DevOps success. We present how automated performance benchmarks may be included into continuous integration. As an example, we report on regression benchmarks for application monitoring frameworks and illustrate the inclusion of automated benchmarks into continuous integration setups.},
  keywords = {_tablet,Jenkins,Kieker,MooBench},
  file = {C\:\\Users\\master\\Zotero\\storage\\L5MUF3HG\\Waller et al_2015_Including Performance Benchmarks into Continuous Integration to Enable DevOps.pdf}
}

@inproceedings{xanthopoulosPuttingHumanBack2020,
  title = {Putting the {{Human Back}} in the {{AutoML Loop}}},
  author = {Xanthopoulos, Iordanis and Tsamardinos, I. and Christophides, V. and Simon, Eric and Salinger, Alejandro},
  date = {2020},
  url = {https://www.semanticscholar.org/paper/Putting-the-Human-Back-in-the-AutoML-Loop-Xanthopoulos-Tsamardinos/7293b51020d422ff14515abc7c91962713ea8391},
  urldate = {2023-01-25},
  abstract = {Automated Machine Learning (AutoML) is a rapidly rising subfield of Machine Learning. AutoML aims to fully automate the machine learning process end-to-end, democratizing Machine Learning to non-experts and drastically increasing the productivity of expert analysts. So far, most comparisons of AutoML systems focus on quantitative criteria such as predictive performance and execution time. In this paper, we examine AutoML services for predictive modeling tasks from a user’s perspective, going beyond predictive performance. We present a wide palette of criteria and dimensions on which to evaluate and compare these services as a user. This qualitative comparative methodology is applied on seven AutoML systems, namely Auger.AI, BigML, H2O’s Driverless AI, Darwin, Just Add Data Bio, RapidMiner, and Watson. The comparison indicates the strengths and weaknesses of each service, the needs that it covers, the segment of users that is most appropriate for, and the possibilities for improvements.},
  eventtitle = {{{EDBT}}/{{ICDT Workshops}}},
  file = {C\:\\Users\\master\\Zotero\\storage\\SA4IQ5AH\\Xanthopoulos et al_2020_Putting the Human Back in the AutoML Loop.pdf}
}

@article{yangHyperparameterOptimizationMachine2020,
  title = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}: {{Theory}} and {{Practice}}},
  shorttitle = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2020-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {415},
  eprint = {2007.15745},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {295--316},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.07.061},
  url = {http://arxiv.org/abs/2007.15745},
  urldate = {2023-01-25},
  abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
  keywords = {68T01; 90C31,C.2.0,Computer Science - Machine Learning,I.2.0,I.2.2,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\6TZA7J63\\Yang_Shami_2020_On Hyperparameter Optimization of Machine Learning Algorithms.pdf;C\:\\Users\\master\\Zotero\\storage\\CNXX8V4E\\2007.html}
}

@article{yangIoTDataAnalytics2022,
  title = {{{IoT Data Analytics}} in {{Dynamic Environments}}: {{From An Automated Machine Learning Perspective}}},
  shorttitle = {{{IoT Data Analytics}} in {{Dynamic Environments}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2022-11},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {116},
  eprint = {2209.08018},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {105366},
  issn = {09521976},
  doi = {10.1016/j.engappai.2022.105366},
  url = {http://arxiv.org/abs/2209.08018},
  urldate = {2023-01-25},
  abstract = {With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.},
  keywords = {68T01; 90C31,C.2.0,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Systems and Control,I.2.0,I.2.2},
  file = {C\:\\Users\\master\\Zotero\\storage\\MHGKKJW5\\Yang_Shami_2022_IoT Data Analytics in Dynamic Environments.pdf;C\:\\Users\\master\\Zotero\\storage\\AFUU3C7X\\2209.html}
}

@article{zahariaAcceleratingMachineLearning2018,
  title = {Accelerating the {{Machine Learning Lifecycle}} with {{MLflow}}},
  author = {Zaharia, M. and Chen, A. and Davidson, A. and Ghodsi, A. and Hong, S. and Konwinski, A. and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
  date = {2018},
  journaltitle = {IEEE Data Eng. Bull.},
  url = {https://www.semanticscholar.org/paper/Accelerating-the-Machine-Learning-Lifecycle-with-Zaharia-Chen/b2e0b79e6f180af2e0e559f2b1faba66b2bd578a},
  urldate = {2023-03-14},
  abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
  file = {C\:\\Users\\master\\Zotero\\storage\\6RA7FM6Z\\Zaharia et al_2018_Accelerating the Machine Learning Lifecycle with MLflow.pdf}
}
