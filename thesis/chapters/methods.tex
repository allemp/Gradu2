\chapter{Methods}
\label{chap:methods}

%--------------------------------------------------------------------------------------------------------------------------------------------
\section{Research setup}
\subsection{Scope}

In this thesis the choice of machine learning algorithms is limited to implementations with iterative training and a possibility for metric collection between training steps.

The scope of the study is limited to 5 performance metrics of 3 different ML models trained and tested on 3 different datasets using a distributed computing framework Ray Tune \parencite{liawTuneResearchPlatform2018}.

TODO different models, different datasets

TODO Vertailukriteeristö: tapana ohjelmistopuolella + tapana koneoppimispuolella

TODO different resources (memory, time, accuracy)

%\begin{itemize}
%    \item Task Completion time
%    \item Throughput
%    \item Latency
%    \item CPU usage
%    \item GPU usage
%    \item RAM usage
%    \item VRAM usage
%    \item I/O usage
%    \item Network traffic
%\end{itemize}


\subsection{Research Questions}
This master's thesis asks the following research questions:
\begin{itemize}
    \item \emph{RQ1}: How do changes in hyperparameters affect system performance during model training?
    \item \emph{RQ2}: How does early stopping on system performance criteria affect computational budgets during model training?

\end{itemize}

\subsection{Proposed method}




\section{Experimental setup}

\subsection{Methodology}
TODO Methodology used is expanded from an existing methodology for machine learning experiment design \parencite{fernandez-lozanoMethodologyDesignExperiments2016} to include AutoML and

\subsection{Software and Hardware}

Experiments were performed using Ray Tune (2.7.1) \parencite{liawTuneResearchPlatform2018}. MLFlow (2.7.1) \parencite{chenDevelopmentsMLflowSystem2020} was used for collecting metrics and tracking experiments. Scikit-learn (1.3.2) \parencite{pedregosaScikitlearnMachineLearning2011} for training and evaluating machine learning models. Hardware consisted of TODO processor and Nvidia 3060 GPU.

\subsection{Datasets}
% Tarkemmin esimerkkinä, loput vähemmän tarkasti
MNIST \parencite{dengMNISTDatabaseHandwritten2012}

% Satunnaisesti valitaan X kappaletta
Penn Machine Learning Benchmarks \parencite{olsonPMLBLargeBenchmark2017}
\subsection{Metrics}
\subsection{Algorithms}

Chosen algorithms shown in table \ref{table:algorithms} support training in batches and are based on stochastic gradient descent implementations in the Scikit-learn \parencite{pedregosaScikitlearnMachineLearning2011}.

\begin{table}[h]
    \label{table:algorithms}
    \centering
    \begin{tabular}{lll}
        \toprule
        Algorithm                    & Type           & Loss          \\
        \midrule
        Linear regression            & regression     & squared\_loss \\
        Logistic regression          & classification & log           \\
        Support Vector Machine (SVM) & classification & hinge         \\
        \bottomrule
    \end{tabular}
\end{table}



\subsection{Validation}


%--------------------------------------------------------------------------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
%\subsection{Overview}

\subsection{Preliminary experiments}
\subsection{System performance}
\subsection{Computational budget}

%--------------------------------------------------------------------------------------------------------------------------------------------
