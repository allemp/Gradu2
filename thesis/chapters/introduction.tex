\chapter{Introduction}
\label{chap:intro}

%\info[inline]{Intro paragraph}

Machine learning (ML) and Artificial Intelligence (AI) have been a hot topic of discussion in the past decade. While there is a mountain of academic research on ML methods and tools, there is a lack of attention paid to practical real-world challenges encountered when developing or running ML systems. DevOps has previously addressed similar challenges in software engineering and a field of MLOps which is DevOps applied to ML has emerged. Machine learning operations (MLOps) focuses on solving challenges related to operating real world machine learning systems \parencite{kreuzbergerMachineLearningOperations2023}.

%\info[inline]{Context of the study}
Real world machine learning (ML) systems are widely deployed in production in a wide range of domains \parencite{cabreraRealworldMachineLearning2023}.
Examples of machine learning systems in different fields include recommender systems \parencite{liRecentDevelopmentsRecommender2023}, targeted ads \parencite{domingosFewUsefulThings2012}, drug design \parencite{domingosFewUsefulThings2012} or search engines \parencite{domingosFewUsefulThings2012}.

Most recent breakthroughs that have generated media attention have been in the fields of computer vision in the form of latent diffusion models (LDM) \parencite{rombachHighResolutionImageSynthesis2022} such as Stable Diffusion \parencite{stabilityaiStableDiffusionPublic2022} for generating images from prompts and natural language processing in the form of large language models (LLM) \parencite{touvronLLaMAOpenEfficient2023} such as ChatGPT \parencite{openaiIntroducingChatGPT2022}. There have also been great developments in tooling for machine learning such as Tensorflow \parencite{abadiTensorFlowLargeScaleMachine2016}, Pytorch \parencite{paszkePyTorchImperativeStyle2019} or scikit-learn \parencite{pedregosaScikitlearnMachineLearning2011} for model development, Ray \parencite{liawTuneResearchPlatform2018}, Horovod \parencite{sergeevHorovodFastEasy2018} or DeepSpeed \parencite{rasleyDeepSpeedSystemOptimizations2020} for distributed training and MLFlow \parencite{chenDevelopmentsMLflowSystem2020} or Tensorboard \parencite{abadiTensorFlowLargeScaleMachine2016} for machine learning monitoring.

%\info[inline]{Statement of the problem}

Despite wide adoption and many successes, there are still challenges with machine learning systems in practice \parencite{daiAddressingModernPractical2023}. The required amount of computation for machine learning has been on the rise \parencite{sarkerMachineLearningAlgorithms2021} and in particular the amount of incoming data has required new solutions such as distributed or federated learning \parencite{daiAddressingModernPractical2023}. Realistic computational budgets and practical efficiency in real world scenarios have only recently been started to be researched \parencite{prabhuComputationallyBudgetedContinual2023}. According to an OpenAI technical blog the trend is exponential and more compute leads to better performance \parencite{amodeiAICompute2018}. Increased compute requirements also mean increased costs such as financial, operational or environmental. Strubell et al. \parencite*{strubellEnergyPolicyConsiderations2020} in their extended abstract bring attention to the environmental impact of training models and in particular hyperparameter tuning, during which costs of training many relatively inexpensive models quickly adds up.

In addition to cost there may be other requirements for machine learning systems. For example machine learning systems on the edge might encounter system requirements such as latency and energy use or have limited resources such as memory or compute \parencite{chenDeepLearningEdge2019}. Ways of meeting these requirements include hyperparameter tuning, reducing the amount of parameters in the model or model compression such as knowledge distillation \parencite{chenDeepLearningEdge2019}.

Early stopping has been used as a cost optimization technique to reduce training time by stopping training when performance of the model stops improving on the validation set \parencite{precheltAutomaticEarlyStopping1998}. More recent work on larger models shows that models might still improve later if training continues for a longer time \parencite{hofferTrainLongerGeneralize2018}. Using early stopping with other performance metrics such as system metrics has not been as thoroughly studied.

%\info[inline]{Aim and Scope}

The aim of this thesis is to investigate whether using early stopping with system metrics leads to more efficient hyperparameter tuning when there are resource constraints. Investigation is limited to a small set of widely available machine learning algorithms and datasets that do not require a lot of computation. While more complex and effective hyperparameter optimization methods exist only the simplest are used to simplify the experiments for clarity. The theoretical significance of the thesis is to show that traditional hyperparameter optimization techniques can not only be used on machine learning performance metrics but also alternative metrics such as system metrics. The practical outcomes are reducing costs and allowing for quickly and efficiently tailoring models to fit specific system metric constrains.
%\info[inline]{Overview}

This thesis is structured in the following manner: Chapter \ref{chap:mloper} contains background information about machine learning, DevOps and MLOps and how they relate to each other. Chapter \ref{chap:methods} describes the performed experiments and their methods and design including research questions, datasets and algorithms used and concludes with the results of the experiments. Chapter \ref{chap:discuss} revisits the research questions and discusses the interpretation of the results, limitations, related work and future work. Chapter \ref{chap:concl} concludes the thesis by summarizing key findings.