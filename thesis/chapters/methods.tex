\chapter{Methods}
\label{chap:methods}

%--------------------------------------------------------------------------------------------------------------------------------------------
\section{Research setup}
\subsection{Scope}

In this thesis the choice of machine learning algorithms is limited to implementations with iterative training and a possibility for metric collection between training steps.

The scope of the study is limited to $5$ performance metrics of $3$ different ML models trained and tested on 3 different datasets using a distributed computing framework Ray Tune \parencite{liawTuneResearchPlatform2018}.

TODO different models, different datasets

TODO Vertailukriteeristö: tapana ohjelmistopuolella + tapana koneoppimispuolella

TODO different resources (memory, time, accuracy)

%\begin{itemize}
%    \item Task Completion time
%    \item Throughput
%    \item Latency
%    \item CPU usage
%    \item GPU usage
%    \item RAM usage
%    \item VRAM usage
%    \item I/O usage
%    \item Network traffic
%\end{itemize}


\subsection{Research Questions}
This master's thesis asks the following research questions:
\begin{itemize}
    \item \emph{RQ1}: How do changes in hyperparameters affect system performance during model training?
    \item \emph{RQ2}: How does early stopping on system performance criteria affect computational budgets during model training?

\end{itemize}

\subsection{Methodology}
TODO Methodology used is expanded from an existing methodology for machine learning experiment design \parencite{fernandez-lozanoMethodologyDesignExperiments2016} to include AutoML and

\section{Experimental setup}
\unsure[inline]{Kannattaako tässä osiossa perustella valintoja?}

\subsection{Software and Hardware}

Experiments were performed using Ray Tune (2.7.1) \parencite{liawTuneResearchPlatform2018}. MLFlow (2.7.1) \parencite{chenDevelopmentsMLflowSystem2020} was used for collecting metrics and tracking experiments. Scikit-learn (1.3.2) \parencite{pedregosaScikitlearnMachineLearning2011} for training and evaluating machine learning models. Hardware used to perform the experiments consisted of TODO processor and Nvidia 3060 GPU.

\subsection{Datasets}

Datasets used were chosen from Penn Machine Learning Benchmarks \parencite{olsonPMLBLargeBenchmark2017}. Mnist dataset was chosen because of popularity and familiarity in the machine learning community. The rest of the datasets were chosen by hand instead of randomly sampled to represent small and large datasets in both classification and regression tasks, because the repository is unbalanced and contains a lot of very similar datasets. Table~\ref{table:datasets} summarizes the machine learning task and dimensionality of the datasets. The datasets are loaded using the Penn Machine Learning Benchmark \parencite{olsonPMLBLargeBenchmark2017} python package and consist of input features and a target variable.

\begin{table}[h]
    \centering
    \begin{tabular}{lllll}
        \toprule
        Dataset        & Type    & Task           & Examples & Features \\
        \midrule
        mnist          & image   & classification & 70000    & 784      \\
        kddcup         & tabular & classification & 494020   & 41       \\
        diabetes       & tabular & classification & 768      & 8        \\
        1191\_BNG\_pbc & tabular & regression     & 1000000  & 19       \\
        529\_pollen    & tabular & regression     & 3848     & 5        \\
        \bottomrule
    \end{tabular}
    \caption{Summary of the datasets used.}
    \label{table:datasets}
\end{table}


\subsection{Algorithms}

Algorithms were chosen to support training in batches without being computationally heavy. Linear regression, logistic regression and support vector machine (SVM) are based on stochastic gradient descent (SGD) implementation found in Scikit-learn \parencite{pedregosaScikitlearnMachineLearning2011}. Algorithms are summarized in Table~\ref{table:algorithms}.

\begin{table}[h]
    \centering
    \begin{tabular}{lll}
        \toprule
        Algorithm              & Loss    & Hyperparameters                  \\
        \midrule
        Linear regression      & squared & batch size, learning rate, alpha \\
        Logistic regression    & log     & batch size, learning rate, alpha \\
        Support Vector Machine & hinge   & batch size, learning rate, alpha \\
        \bottomrule
    \end{tabular}
    \caption{Summary of the algorithms}
    \label{table:algorithms}
\end{table}

\subsection{Metrics}
Main metrics to be evaluated can be divided into machine learning metrics and system performance metrics and are summarized in Table~\ref{table:metrics}. Training loss was computed with each training step and the rest of the metrics were computed every $100$ steps
% https://docs.ray.io/en/latest/ray-core/scheduling/memory-management.html#memory
\begin{table}[h]
    \centering
    \begin{tabular}{llll}
        \toprule
        Metric                     & Type               \\
        \midrule
        training loss              & machine learning   \\
        validation loss            & machine learning   \\
        accuracy                   & machine learning   \\
        root mean square error     & machine learning   \\
        average training step time & system performance \\
        total training time        & system performance \\
        cpu util                   & system performance \\
        memory util                & system performance \\

        \bottomrule
    \end{tabular}
    \caption{Summary of the metrics}
    \label{table:metrics}
\end{table}


\subsection{Validation}

Machine learning models were validated by splitting the dataset into a $70\%$ training set and a $30\%$ test set. To make sure that measurements are not sensitive to chosen data five-fold cross validation using the training set was performed when tuning hyperparameters.

%--------------------------------------------------------------------------------------------------------------------------------------------
\section{Experiments}
\label{sec:experiments}
%\subsection{Overview}

\subsection{Preliminary experiments}
\subsection{System performance}
\subsection{Computational budget}

%--------------------------------------------------------------------------------------------------------------------------------------------
