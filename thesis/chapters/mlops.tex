\chapter{Machine Learning Operations}
\label{chap:mloper}

\improvement[inline]{Add outline of how ML, DevOps and MLOps fit together, add scope}

%--------------------------------------------------------------------------------------------------------------------------------------------
\section{Machine Learning} % 2 pages
\label{sec:ml}
\improvement[inline]{Add outline of how Practical ML and ML metrics fit together, add scope}

\subsection{Practical machine learning \unsure{Instead of Overview}}

\info[inline]{Intro to ML} % Definition and basics of ML
Writing programs and developing algorithms to complete specific tasks is a labor intensive task requiring professional programming expertise.
A different approach is to develop generic algorithms that can change behavior by learning.
The field studying these types of algorithms is called machine learning.
Machine learning algorithms learn by applying an optimization algorithm to adjust set of parameters called a model and this process is called training the model \parencite{lecunDeepLearning2015}. % TODO find reference
A simplified machine learning workflow consists of splitting the data into training and test datasets, performing preprocessing separately for each dataset, training the model on the training dataset and finally evaluating the trained model on the test dataset\todo{ref}.  % TODO find reference


Machine learning is widely used in applications like search, drug design or ad placement and can be also known as data mining or predictive analytics \parencite{domingosFewUsefulThings2012}. Developing machine learning systems, which are systems that are based on machine learning, can be a difficult task. Unlike traditional software development, experiments with both code and data as inputs are central to machine learning development \parencite{zahariaAcceleratingMachineLearning2018} and reproducibility of the experiments is often problematic. While plenty of research focuses on machine learning methods or even datasets and data quality, the biggest bottleneck is human cycles \parencite{domingosFewUsefulThings2012}. Faster iterations improve the machine learning developer or researcher experience. An important metric to pay attention to and optimize is the mean iteration cycle for machine learning developers.

Machine learning can be practiced with two different goals in mind. First is explanatory modeling with the purpose of scientific theory building and testing and the second is predictive modeling mostly used outside of scientific research \parencite{shmueliExplainPredict2010a}.
One practical difference is that unlike predictive modeling, explanatory modeling rarely uses holdout test sets or cross validation for evaluation \parencite{shmueliExplainPredict2010a}.
Lack or presence of evaluation on a test set can be used as a heuristic to quickly determine whether a machine learning project is explanatory or predictive in nature.
However, even explanatory modeling benefits from evaluating the predictive power \parencite{shmueliExplainPredict2010a}.
Domingos \parencite*{domingosFewUsefulThings2012} in their paper assume all machine learning is predictive in nature and state that machine learning should generalize beyond the training set \unsure{Direct quote paraphrased}.
It is important to keep in mind the end goals of a machine learning project, because common practices in a research setting might not be applicable when creating machine learning systems.

Machine learning algorithms can be categorized as supervised learning, unsupervised learning or reinforcement learning\todo{ref}. %TODO find reference
The main differences are related to whether the model learns by using "right answers" provided by labeled data in supervised learning, by finding structure in the dataset in unsupervised learning or by interacting with the world in reinforcement learning\todo{ref}. %TODO find reference
Model evaluation is also different with supervised learning mostly relying on universal cross-validation on previously unseen data and unsupervised or reinforcement learning relying on internal evaluation metrics tied to the specific algorithm\todo{ref}. %TODO find reference  
Unsupervised learning has the advantage of not requiring labeled data which is an advantage for problems where labels are uncommon \parencite{leBuildingHighlevelFeatures2012}.


\subsection{Machine learning performance metrics}
\label{sec:mlperf}
\info[inline]{Intro}
\info[inline]{Training vs Validation metrics}
\info[inline]{Domain specific metrics}


%--------------------------------------------------------------------------------------------------------------------------------------------
\section{DevOps} % 2 pages
\label{sec:devops}

\subsection{Benefits of DevOps \unsure{Instead of overview}}

A common interpretation of DevOps is a focus on software quality, collaboration between development and operations, process speed and rapid feedback \parencite{mishraDevOpsSoftwareQuality2020,wallerIncludingPerformanceBenchmarks2015, pereraImproveSoftwareQuality2017}. Defining DevOps is difficult as there is no consensus on the exact definition \parencite{smedsDevOpsDefinitionPerceived2015,mishraDevOpsSoftwareQuality2020}. DevOps can be viewed from different points of view such as culture, collaboration, automation, measurements and monitoring \parencite{mishraDevOpsSoftwareQuality2020, wallerIncludingPerformanceBenchmarks2015}. In DevOps there is a focus on speed and quality with incremental changes that are recurrent and continuous \parencite{mishraDevOpsSoftwareQuality2020}. The goal is to bridge the gap between development and operations \parencite{smedsDevOpsDefinitionPerceived2015}. This is done through sharing tasks and responsibilities from development to deployment and support \parencite{mishraDevOpsSoftwareQuality2020}.

Continuous integration, continuous deployment and continuous monitoring are well known practices in DevOps \parencite{wallerIncludingPerformanceBenchmarks2015} describing the automatic nature of integrating, deploying and monitoring code changes. Feedback includes performance metrics data which is then fed as an input during planning and development \parencite{smedsDevOpsDefinitionPerceived2015}. Performance profiling and monitoring are similar activities and the main difference is whether it's done during the development process or during operations respectively \parencite{wallerIncludingPerformanceBenchmarks2015} with DevOps bridging the gap between them \parencite{brunnertPerformanceorientedDevOpsResearch2015}. Continuous benchmarking allows for detecting performance regressions during continuous integration \parencite{wallerIncludingPerformanceBenchmarks2015} and infrastructure monitoring with a feedback loop allows for performance optimization in production \parencite{smedsDevOpsDefinitionPerceived2015}.

Performance evaluation is a useful tool for optimizing the overall system design and tailoring for a specific production environment in addition to correctly sizing resources \parencite{brunnertPerformanceorientedDevOpsResearch2015,wallerIncludingPerformanceBenchmarks2015}. \unsure{Moved sentence from previous paragraph} Resource demands might change depending on the inputs \parencite{brunnertPerformanceorientedDevOpsResearch2015} making it important to systematically measure performance not only based on code changes but also on configuration changes or even data changes. Performance evaluation is directly tied to defining and collecting performance metrics and monitoring.

\subsection{Software performance metrics}

Performance metrics are fundamental to all activities involving performance evaluation such as profiling or monitoring \parencite{brunnertPerformanceorientedDevOpsResearch2015}. Common metrics involve measuring the CPU, but other metrics such as memory usage, network traffic or I/O usage do not have clear definitions \parencite{brunnertPerformanceorientedDevOpsResearch2015}. Collecting metrics happens through hardware based monitors or software monitors instrumented into software through code modification or indirectly for example through middleware interception \parencite{brunnertPerformanceorientedDevOpsResearch2015}. Metrics can be event driven in which a monitor is triggered with every occurrence or based on sampling at fixed time intervals \parencite{brunnertPerformanceorientedDevOpsResearch2015}. The types of metrics collected and what information is expected depends on the performance goals and the life cycle of the software \parencite{brunnertPerformanceorientedDevOpsResearch2015}.

Metrics can be divided into application metrics such as response time or throughput and resource utilization metrics such as CPU utilization or available memory \parencite{brunnertPerformanceorientedDevOpsResearch2015}. There is little peer reviewed research available with specifics on which metrics are to be collected or how they are defined. \improvement{Find non-research paper, state of devops?} \improvement{Find paper on monitoring?}

Measurement based performance evaluation requires a system to test while model based performance evaluation allows to predict the performance of the future system \parencite{brunnertPerformanceorientedDevOpsResearch2015}. This type of performance prediction allows for better planning and comparing use cases especially when an existing legacy system exists with measured performance metrics \parencite{brunnertPerformanceorientedDevOpsResearch2015}.

%--------------------------------------------------------------------------------------------------------------------------------------------
\section{MLOps} % 4 pages
\label{sec:mldevops}

\subsection{Production machine learning systems \unsure{Instead of overview}}

While the focus of machine learning research has been on improving models, it is essential for the industry to be able to design production-ready machine learning pipelines \parencite{posoldovaMachineLearningPipelines2020}. The data often used for research is of higher quality than real-world data that is often messy, unstructured and unlabeled \parencite{posoldovaMachineLearningPipelines2020}. Continuous integration, continuous deployment and automated testing are also relevant to machine learning systems \parencite{posoldovaMachineLearningPipelines2020} which are familiar concepts from DevOps. A new concept of MLOps addresses this issue of designing and maintaining machine learning systems just like DevOps addressed it for traditional software \parencite{kreuzbergerMachineLearningOperations2023}.

Managing technical debt is even more important in machine learning systems, because of machine learning specific issues that cannot be solved with traditional methods \parencite{sculleyHiddenTechnicalDebt2015a}. Main culprit for the challenges with machine learning systems is that data changes the behavior of the system and cannot be expressed with code alone \parencite{sculleyHiddenTechnicalDebt2015a}. Challenges like entanglement, correction cascades or feedback loops are common with machine learning systems and are difficult to diagnose with common tools \parencite{sculleyHiddenTechnicalDebt2015a}.

Requirements for a machine learning system are different depending on the task. For example speech and object recognition might have no particular performance requirements during training but has strict latency and computational resource restrictions when deployed to serve large amounts users \parencite{hintonDistillingKnowledgeNeural2015}. MLOps has to take into account both machine learning performance metrics familiar from machine learning and software performance metrics familiar from DevOps and software engineering. Feedback from metrics collected during development and from monitoring of production systems are core MLOps principles \parencite{kreuzbergerMachineLearningOperations2023}. For example possible meta-level requirements include users requesting data deletion, prohibitions on specific features like age or deperecated sources \parencite{breckMLTestScore2017a}.

Performance measuring software is not new, but ML brings additional challenges in the form of models and data which requires a modified approach \parencite{breckMLTestScore2017a}. It is also important to note, that not every data scientist or machine learning engineer working on machine learning systems has a software engineering background \parencite{finzerDataScienceEducation2013} and might lack the necessary knowledge to apply software engineering best practices to machine learning systems. Monitoring for machine learning systems has to be carefully designed \parencite{sculleyHiddenTechnicalDebt2015a}. Hyperparameter optimization is a kind of performance optimization, where the goal is to improve machine learning metrics. It is not always necessary to train the model to completion to verify that training code is correct and training loss is decreasing \parencite{breckMLTestScore2017a}.


\subsection{Hyperparameter optimization}

% Hyperparameter definition and examples

Parameters given as part of a configuration to the machine learning model are called hyperparameters \parencite{yangHyperparameterOptimizationMachine2020}. \improvement{ML Test paper why + examples}

%For example learning rate, batch size or a classification threshold are hyperparameters set before the model is trained.
%\todo{ref} Most common hyperparameter selection technique in practice is manually adjusting the hyperparameters and is humorously called graduate student descent\todo{ref}. % TODO find reference

% Benefits of hyperparameter search
Automatic tuning of hyperparameters can help achieve state-of-the-art performance in machine learning systems \parencite{maclaurinGradientbasedHyperparameterOptimization2015}.
Hyperparameter optimization techniques include grid search, random search, gradient based optimization and Bayesian optimization and they have different benefits and limitations \parencite{yangHyperparameterOptimizationMachine2020}.

Similar concepts to hyperparameter optimization are neural architecture optimization and meta modeling where model structure or modeling algorithm is treated as a tunable parameter \parencite{bakerAcceleratingNeuralArchitecture2017}.
For example a machine learning system can automate the choosing of the number of neural network layers, the type of layers or even the type of machine learning algorithm.
Tuning hyperparameters is generally a difficult task \parencite{maclaurinGradientbasedHyperparameterOptimization2015}.

Traditional hyperparameter tuning methods such as Bayesian optimization are unfeasible for more than 10-20 hyperparameters \parencite{maclaurinGradientbasedHyperparameterOptimization2015}.
More advanced techniques are required if a larger amount of tunable hyperparameters is desired.
Performance prediction is an important step to reduce the amount of computation required for neural architecture search and hyperparameter optimization \parencite{bakerAcceleratingNeuralArchitecture2017}.

\subsection{Performance prediction and early stopping}

Early stopping is a technique in which model training is halted before completion to avoid wasting computational resources \parencite{precheltAutomaticEarlyStopping1998}.

Machine learning systems in addition to machine learning performance metrics and system performance metrics will have their performance metrics tied to product or organization metrics such as user churn rate or click-through rate \parencite{shankarOperationalizingMachineLearning2022}. Choosing the right metrics to evaluate a machine learning system is important and the metrics will be different for different machine learning systems \parencite{shankarOperationalizingMachineLearning2022}.