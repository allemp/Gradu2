@misc{bakerAcceleratingNeuralArchitecture2017,
  title = {Accelerating {{Neural Architecture Search}} Using {{Performance Prediction}}},
  author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  date = {2017-11-08},
  number = {arXiv:1705.10823},
  eprint = {1705.10823},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1705.10823},
  url = {http://arxiv.org/abs/1705.10823},
  urldate = {2023-01-25},
  abstract = {Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\master\\Zotero\\storage\\I93LMDHA\\Baker et al_2017_Accelerating Neural Architecture Search using Performance Prediction.pdf;C\:\\Users\\master\\Zotero\\storage\\IWHABXLB\\1705.html}
}

@inproceedings{breckMLTestScore2017a,
  title = {The {{ML Test Score}}: {{A Rubric}} for {{ML Production Readiness}} and {{Technical Debt Reduction}}},
  shorttitle = {The {{ML Test Score}}},
  booktitle = {Proceedings of {{IEEE Big Data}}},
  author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
  date = {2017},
  keywords = {_tablet},
  file = {C\:\\Users\\master\\Zotero\\storage\\LEKCIDWU\\Breck et al_2017_The ML Test Score.pdf}
}

@misc{brunnertPerformanceorientedDevOpsResearch2015,
  title = {Performance-Oriented {{DevOps}}: {{A Research Agenda}}},
  shorttitle = {Performance-Oriented {{DevOps}}},
  author = {Brunnert, Andreas and van Hoorn, Andre and Willnecker, Felix and Danciu, Alexandru and Hasselbring, Wilhelm and Heger, Christoph and Herbst, Nikolas and Jamshidi, Pooyan and Jung, Reiner and von Kistowski, Joakim and Koziolek, Anne and Kroß, Johannes and Spinner, Simon and Vögele, Christian and Walter, Jürgen and Wert, Alexander},
  options = {useprefix=true},
  date = {2015-08-18},
  number = {arXiv:1508.04752},
  eprint = {1508.04752},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1508.04752},
  url = {http://arxiv.org/abs/1508.04752},
  urldate = {2023-01-17},
  abstract = {DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. Performance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Performance,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\9MDJPVS9\\Brunnert et al_2015_Performance-oriented DevOps.pdf;C\:\\Users\\master\\Zotero\\storage\\3DKEU57U\\1508.html}
}

@inproceedings{cardososilvaBenchmarkingMachineLearning2020,
  title = {Benchmarking {{Machine Learning Solutions}} in {{Production}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Cardoso Silva, Lucas and Rezende Zagatti, Fernando and Silva Sette, Bruno and Nildaimon dos Santos Silva, Lucas and Lucrédio, Daniel and Furtado Silva, Diego and de Medeiros Caseli, Helena},
  options = {useprefix=true},
  date = {2020-12},
  pages = {626--633},
  doi = {10.1109/ICMLA51294.2020.00104},
  abstract = {Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.},
  eventtitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  keywords = {_tablet,Benchmark,Benchmark testing,Machine learning,Machine Learning,MLOps,Monitoring,Production,Systems operation,Task analysis,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\7WJUB6UG\\Cardoso Silva et al_2020_Benchmarking Machine Learning Solutions in Production.pdf;C\:\\Users\\master\\Zotero\\storage\\3PXPYX5A\\9356298.html}
}

@misc{coteQualityIssuesMachine2022,
  title = {Quality Issues in {{Machine Learning Software Systems}}},
  author = {Côté, Pierre-Olivier and Nikanjam, Amin and Bouchoucha, Rached and Khomh, Foutse},
  date = {2022-08-22},
  number = {arXiv:2208.08982},
  eprint = {2208.08982},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.08982},
  url = {http://arxiv.org/abs/2208.08982},
  urldate = {2022-12-07},
  abstract = {Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threat to human life. The quality assurance of MLSSs is considered as a challenging task and currently is a hot research topic. Moreover, it is important to cover all various aspects of the quality in MLSSs. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of bad-practices related to poor quality in MLSSs. Method: We plan to conduct a set of interviews with practitioners/experts, believing that interviews are the best method to retrieve their experience and practices when dealing with quality issues. We expect that the catalog of issues developed at this step will also help us later to identify the severity, root causes, and possible remedy for quality issues of MLSSs, allowing us to develop efficient quality assurance tools for ML models and MLSSs.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\NPNEQEVA\\Côté et al_2022_Quality issues in Machine Learning Software Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\ES8GKMEI\\2208.html}
}

@article{finzerDataScienceEducation2013,
  title = {The {{Data Science Education Dilemma}}},
  author = {Finzer, William},
  date = {2013},
  journaltitle = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {2},
  doi = {10.5070/T572013891},
  url = {https://escholarship.org/uc/item/7gv0q9dc},
  urldate = {2023-01-17},
  abstract = {The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter-disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.},
  langid = {english},
  keywords = {_tablet},
  file = {C\:\\Users\\master\\Zotero\\storage\\YWX2L6LE\\Finzer_2013_The Data Science Education Dilemma.pdf}
}

@misc{kreuzbergerMachineLearningOperations2022,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  date = {2022-05-14},
  number = {arXiv:2205.02302},
  eprint = {2205.02302},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.02302},
  url = {http://arxiv.org/abs/2205.02302},
  urldate = {2022-12-07},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\XUXSDP43\\Kreuzberger et al_2022_Machine Learning Operations (MLOps).pdf;C\:\\Users\\master\\Zotero\\storage\\6LMZFC2P\\2205.html}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2022-09-06},
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.02497},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-12-07},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\278JM9KT\\Lones_2022_How to avoid machine learning pitfalls.pdf;C\:\\Users\\master\\Zotero\\storage\\6RWMHTU8\\2108.html}
}

@article{mishraDevOpsSoftwareQuality2020,
  title = {{{DevOps}} and Software Quality: {{A}} Systematic Mapping},
  shorttitle = {{{DevOps}} and Software Quality},
  author = {Mishra, Alok and Otaiwi, Ziadoon},
  date = {2020-11-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {38},
  pages = {100308},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100308},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013720304081},
  urldate = {2023-01-17},
  abstract = {Quality pressure is one of the factors affecting processes for software development in its various stages. DevOps is one of the proposed solutions to such pressure. The primary focus of DevOps is to increase the deployment speed, frequency and quality. DevOps is a mixture of different developments and operations to its multitudinous ramifications in software development industries, DevOps have attracted the interest of many researchers. There are considerable literature surveys on this critical innovation in software development, yet, little attention has been given to DevOps impact on software quality. This research is aimed at analyzing the implications of DevOps features on software quality. DevOps can also be referred to a change in organization cultures aimed at removal of gaps between the development and operations of an organization. The adoption of DevOps in an organization provides many benefits including quality but also brings challenges to an organization. This study presents systematic mapping of the impact of DevOps on software quality. The results of this study provide a better understanding of DevOps on software quality for both professionals and researchers working in this area. The study shows research was mainly focused in automation, culture, continuous delivery, fast feedback of DevOps. There is need of further research in many areas of DevOps (for instance: measurement, development of metrics of different stages to assess its performance, culture, practices toward ensuring quality assurance, and quality factors such as usability, efficiency, software maintainability and portability).},
  langid = {english},
  keywords = {_tablet,Automation,Development,DevOps,Measurement,Operations,Software,Software quality,Systematic mapping},
  file = {C\:\\Users\\master\\Zotero\\storage\\FI2BPQCJ\\Mishra_Otaiwi_2020_DevOps and software quality.pdf;C\:\\Users\\master\\Zotero\\storage\\NTDWUG9S\\S1574013720304081.html}
}

@misc{shankarOperationalizingMachineLearning2022,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  date = {2022-09-16},
  number = {arXiv:2209.09125},
  eprint = {2209.09125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.09125},
  url = {http://arxiv.org/abs/2209.09125},
  urldate = {2022-12-07},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\K7KRWSL2\\Shankar et al_2022_Operationalizing Machine Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\AWM9SK8B\\2209.html}
}

@article{wallerIncludingPerformanceBenchmarks2015,
  title = {Including {{Performance Benchmarks}} into {{Continuous Integration}} to {{Enable DevOps}}},
  author = {Waller, Jan and Ehmke, Nils C. and Hasselbring, Wilhelm},
  date = {2015-04-03},
  journaltitle = {ACM SIGSOFT Software Engineering Notes},
  shortjournal = {SIGSOFT Softw. Eng. Notes},
  volume = {40},
  number = {2},
  pages = {1--4},
  issn = {0163-5948},
  doi = {10.1145/2735399.2735416},
  url = {https://doi.org/10.1145/2735399.2735416},
  urldate = {2023-01-17},
  abstract = {The DevOps movement intends to improve communication, collaboration, and integration between software developers (Dev) and IT operations professionals (Ops). Automation of software quality assurance is key to DevOps success. We present how automated performance benchmarks may be included into continuous integration. As an example, we report on regression benchmarks for application monitoring frameworks and illustrate the inclusion of automated benchmarks into continuous integration setups.},
  keywords = {_tablet,Jenkins,Kieker,MooBench},
  file = {C\:\\Users\\master\\Zotero\\storage\\L5MUF3HG\\Waller et al_2015_Including Performance Benchmarks into Continuous Integration to Enable DevOps.pdf}
}

@article{yangHyperparameterOptimizationMachine2020,
  title = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}: {{Theory}} and {{Practice}}},
  shorttitle = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2020-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {415},
  eprint = {2007.15745},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {295--316},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.07.061},
  url = {http://arxiv.org/abs/2007.15745},
  urldate = {2023-01-25},
  abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
  archiveprefix = {arXiv},
  keywords = {68T01; 90C31,C.2.0,Computer Science - Machine Learning,I.2.0,I.2.2,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\6TZA7J63\\Yang_Shami_2020_On Hyperparameter Optimization of Machine Learning Algorithms.pdf;C\:\\Users\\master\\Zotero\\storage\\CNXX8V4E\\2007.html}
}

@article{yangIoTDataAnalytics2022,
  title = {{{IoT Data Analytics}} in {{Dynamic Environments}}: {{From An Automated Machine Learning Perspective}}},
  shorttitle = {{{IoT Data Analytics}} in {{Dynamic Environments}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2022-11},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {116},
  eprint = {2209.08018},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  pages = {105366},
  issn = {09521976},
  doi = {10.1016/j.engappai.2022.105366},
  url = {http://arxiv.org/abs/2209.08018},
  urldate = {2023-01-25},
  abstract = {With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.},
  archiveprefix = {arXiv},
  keywords = {68T01; 90C31,C.2.0,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Systems and Control,I.2.0,I.2.2},
  file = {C\:\\Users\\master\\Zotero\\storage\\MHGKKJW5\\Yang_Shami_2022_IoT Data Analytics in Dynamic Environments.pdf;C\:\\Users\\master\\Zotero\\storage\\AFUU3C7X\\2209.html}
}
