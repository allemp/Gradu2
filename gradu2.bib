@inproceedings{cardososilvaBenchmarkingMachineLearning2020,
  title = {Benchmarking {{Machine Learning Solutions}} in {{Production}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Cardoso Silva, Lucas and Rezende Zagatti, Fernando and Silva Sette, Bruno and Nildaimon dos Santos Silva, Lucas and Lucrédio, Daniel and Furtado Silva, Diego and de Medeiros Caseli, Helena},
  options = {useprefix=true},
  date = {2020-12},
  pages = {626--633},
  doi = {10.1109/ICMLA51294.2020.00104},
  abstract = {Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.},
  eventtitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  keywords = {_tablet,Benchmark,Benchmark testing,Machine learning,Machine Learning,MLOps,Monitoring,Production,Systems operation,Task analysis,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\7WJUB6UG\\Cardoso Silva et al_2020_Benchmarking Machine Learning Solutions in Production.pdf;C\:\\Users\\master\\Zotero\\storage\\3PXPYX5A\\9356298.html}
}

@misc{coteQualityIssuesMachine2022,
  title = {Quality Issues in {{Machine Learning Software Systems}}},
  author = {Côté, Pierre-Olivier and Nikanjam, Amin and Bouchoucha, Rached and Khomh, Foutse},
  date = {2022-08-22},
  number = {arXiv:2208.08982},
  eprint = {2208.08982},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.08982},
  url = {http://arxiv.org/abs/2208.08982},
  urldate = {2022-12-07},
  abstract = {Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threat to human life. The quality assurance of MLSSs is considered as a challenging task and currently is a hot research topic. Moreover, it is important to cover all various aspects of the quality in MLSSs. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of bad-practices related to poor quality in MLSSs. Method: We plan to conduct a set of interviews with practitioners/experts, believing that interviews are the best method to retrieve their experience and practices when dealing with quality issues. We expect that the catalog of issues developed at this step will also help us later to identify the severity, root causes, and possible remedy for quality issues of MLSSs, allowing us to develop efficient quality assurance tools for ML models and MLSSs.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\NPNEQEVA\\Côté et al_2022_Quality issues in Machine Learning Software Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\ES8GKMEI\\2208.html}
}

@misc{kreuzbergerMachineLearningOperations2022,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  date = {2022-05-14},
  number = {arXiv:2205.02302},
  eprint = {2205.02302},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.02302},
  url = {http://arxiv.org/abs/2205.02302},
  urldate = {2022-12-07},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we provide an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we furnish a definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\XUXSDP43\\Kreuzberger et al_2022_Machine Learning Operations (MLOps).pdf;C\:\\Users\\master\\Zotero\\storage\\6LMZFC2P\\2205.html}
}

@misc{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2022-09-06},
  number = {arXiv:2108.02497},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.02497},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-12-07},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\278JM9KT\\Lones_2022_How to avoid machine learning pitfalls.pdf;C\:\\Users\\master\\Zotero\\storage\\6RWMHTU8\\2108.html}
}

@misc{shankarOperationalizingMachineLearning2022,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  date = {2022-09-16},
  number = {arXiv:2209.09125},
  eprint = {2209.09125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.09125},
  url = {http://arxiv.org/abs/2209.09125},
  urldate = {2022-12-07},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  archiveprefix = {arXiv},
  keywords = {_tablet,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\K7KRWSL2\\Shankar et al_2022_Operationalizing Machine Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\AWM9SK8B\\2209.html}
}
