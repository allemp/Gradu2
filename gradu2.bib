@online{abadiTensorFlowLargeScaleMachine2016,
  title = {{{TensorFlow}}: {{Large-Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  date = {2016-03-16},
  eprint = {1603.04467},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1603.04467},
  url = {http://arxiv.org/abs/1603.04467},
  urldate = {2024-03-05},
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\ZBPRVPBS\\Abadi et al_2016_TensorFlow.pdf;C\:\\Users\\master\\Zotero\\storage\\XSUZYF7X\\1603.html}
}

@online{amodeiAICompute2018,
  title = {{{AI}} and Compute},
  author = {Amodei, Dario and Hernandez, Danny},
  date = {2018-05-16},
  url = {https://openai.com/research/ai-and-compute},
  urldate = {2023-07-29},
  abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[\^{}footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
  langid = {american},
  file = {C:\Users\master\Zotero\storage\3XICH52X\ai-and-compute.html}
}

@article{aslanpourPerformanceEvaluationMetrics2020,
  title = {Performance Evaluation Metrics for Cloud, Fog and Edge Computing: {{A}} Review, Taxonomy, Benchmarks and Standards for Future Research},
  shorttitle = {Performance Evaluation Metrics for Cloud, Fog and Edge Computing},
  author = {Aslanpour, Mohammad S. and Gill, Sukhpal Singh and Toosi, Adel N.},
  date = {2020-12-01},
  journaltitle = {Internet of Things},
  shortjournal = {Internet of Things},
  volume = {12},
  pages = {100273},
  issn = {2542-6605},
  doi = {10.1016/j.iot.2020.100273},
  url = {https://www.sciencedirect.com/science/article/pii/S2542660520301062},
  urldate = {2024-03-26},
  abstract = {Optimization is an inseparable part of Cloud computing, particularly with the emergence of Fog and Edge paradigms. Not only these emerging paradigms demand reevaluating cloud-native optimizations and exploring Fog and Edge-based solutions, but also the objectives require significant shift from considering only latency to energy, security, reliability and cost. Hence, it is apparent that optimization objectives have become diverse and lately Internet of Things (IoT)-specific born objectives must come into play. This is critical as incorrect selection of metrics can mislead the developer about the real performance. For instance, a latency-aware auto-scaler must be evaluated through latency-related metrics as response time or tail latency; otherwise the resource manager is not carefully evaluated even if it can reduce the cost. Given such challenges, researchers and developers are struggling to explore and utilize the right metrics to evaluate the performance of optimization techniques such as task scheduling, resource provisioning, resource allocation, resource scheduling and resource execution. This is challenging due to (1) novel and multi-layered computing paradigm, e.g., Cloud, Fog and Edge, (2) IoT applications with different requirements, e.g., latency or privacy, and (3) not having a benchmark and standard for the evaluation metrics. In this paper, by exploring the literature, (1) we present a taxonomy of the various real-world metrics to evaluate the performance of cloud, fog, and edge computing; (2) we survey the literature to recognize common metrics and their applications; and (3) outline open issues for future research. This comprehensive benchmark study can significantly assist developers and researchers to evaluate performance under realistic metrics and standards to ensure their objectives will be achieved in the production environments.},
  keywords = {Cloud computing,Cloud Metrics,Fog Computing and Edge Computing,Internet of things (iot),Performance evaluation},
  file = {C\:\\Users\\master\\Zotero\\storage\\6AVP455P\\Aslanpour et al_2020_Performance evaluation metrics for cloud, fog and edge computing.pdf;C\:\\Users\\master\\Zotero\\storage\\9NVW837D\\S2542660520301062.html}
}

@online{bakerAcceleratingNeuralArchitecture2017,
  title = {Accelerating {{Neural Architecture Search}} Using {{Performance Prediction}}},
  author = {Baker, Bowen and Gupta, Otkrist and Raskar, Ramesh and Naik, Nikhil},
  date = {2017-11-08},
  eprint = {1705.10823},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1705.10823},
  url = {http://arxiv.org/abs/1705.10823},
  urldate = {2023-01-25},
  abstract = {Methods for neural network hyperparameter optimization and meta-modeling are computationally expensive due to the need to train a large number of model configurations. In this paper, we show that standard frequentist regression models can predict the final performance of partially trained model configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We empirically show that our performance prediction models are much more effective than prominent Bayesian counterparts, are simpler to implement, and are faster to train. Our models can predict final performance in both visual classification and language modeling domains, are effective for predicting performance of drastically varying model architectures, and can even generalize between model classes. Using these prediction models, we also propose an early stopping method for hyperparameter optimization and meta-modeling, which obtains a speedup of a factor up to 6x in both hyperparameter optimization and meta-modeling. Finally, we empirically show that our early stopping method can be seamlessly incorporated into both reinforcement learning-based architecture selection algorithms and bandit based search methods. Through extensive experimentation, we empirically show our performance prediction models and early stopping algorithm are state-of-the-art in terms of prediction accuracy and speedup achieved while still identifying the optimal model configurations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\master\\Zotero\\storage\\I93LMDHA\\Baker et al_2017_Accelerating Neural Architecture Search using Performance Prediction.pdf;C\:\\Users\\master\\Zotero\\storage\\IWHABXLB\\1705.html}
}

@online{bischlOpenMLBenchmarkingSuites2017,
  title = {{{OpenML Benchmarking Suites}}},
  author = {Bischl, Bernd and Casalicchio, Giuseppe and Feurer, Matthias and Gijsbers, Pieter and Hutter, Frank and Lang, Michel and Mantovani, Rafael G. and van Rijn, Jan N. and Vanschoren, Joaquin},
  options = {useprefix=true},
  date = {2017-08-11},
  url = {https://arxiv.org/abs/1708.03731v3},
  urldate = {2023-11-16},
  abstract = {Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. We advocate the use of curated, comprehensive suites of machine learning tasks to standardize the setup, execution, and reporting of benchmarks. We enable this through software tools that help to create and leverage these benchmarking suites. These are seamlessly integrated into the OpenML platform, and accessible through interfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to use through standardized data formats, APIs, and client libraries; (b) come with extensive meta-information on the included datasets; and (c) allow benchmarks to be shared and reused in future studies. We then present a first, carefully curated and practical benchmarking suite for classification: the OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally, we discuss use cases and applications which demonstrate the usefulness of OpenML benchmarking suites and the OpenML-CC18 in particular.},
  langid = {english},
  organization = {arXiv.org},
  file = {C:\Users\master\Zotero\storage\4E8HAC3Q\Bischl et al_2017_OpenML Benchmarking Suites.pdf}
}

@inproceedings{breckMLTestScore2017a,
  title = {The {{ML Test Score}}: {{A Rubric}} for {{ML Production Readiness}} and {{Technical Debt Reduction}}},
  shorttitle = {The {{ML Test Score}}},
  booktitle = {Proceedings of {{IEEE Big Data}}},
  author = {Breck, Eric and Cai, Shanqing and Nielsen, Eric and Salib, Michael and Sculley, D.},
  date = {2017},
  file = {C:\Users\master\Zotero\storage\LEKCIDWU\Breck et al_2017_The ML Test Score.pdf}
}

@online{brunnertPerformanceorientedDevOpsResearch2015,
  title = {Performance-Oriented {{DevOps}}: {{A Research Agenda}}},
  shorttitle = {Performance-Oriented {{DevOps}}},
  author = {Brunnert, Andreas and van Hoorn, Andre and Willnecker, Felix and Danciu, Alexandru and Hasselbring, Wilhelm and Heger, Christoph and Herbst, Nikolas and Jamshidi, Pooyan and Jung, Reiner and von Kistowski, Joakim and Koziolek, Anne and Kroß, Johannes and Spinner, Simon and Vögele, Christian and Walter, Jürgen and Wert, Alexander},
  options = {useprefix=true},
  date = {2015-08-18},
  eprint = {1508.04752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1508.04752},
  url = {http://arxiv.org/abs/1508.04752},
  urldate = {2023-01-17},
  abstract = {DevOps is a trend towards a tighter integration between development (Dev) and operations (Ops) teams. The need for such an integration is driven by the requirement to continuously adapt enterprise applications (EAs) to changes in the business environment. As of today, DevOps concepts have been primarily introduced to ensure a constant flow of features and bug fixes into new releases from a functional perspective. In order to integrate a non-functional perspective into these DevOps concepts this report focuses on tools, activities, and processes to ensure one of the most important quality attributes of a software system, namely performance. Performance describes system properties concerning its timeliness and use of resources. Common metrics are response time, throughput, and resource utilization. Performance goals for EAs are typically defined by setting upper and/or lower bounds for these metrics and specific business transactions. In order to ensure that such performance goals can be met, several activities are required during development and operation of these systems as well as during the transition from Dev to Ops. Activities during development are typically summarized by the term Software Performance Engineering (SPE), whereas activities during operations are called Application Performance Management (APM). SPE and APM were historically tackled independently from each other, but the newly emerging DevOps concepts require and enable a tighter integration between both activity streams. This report presents existing solutions to support this integration as well as open research challenges in this area.},
  pubstate = {preprint},
  keywords = {Computer Science - Performance,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\9MDJPVS9\\Brunnert et al_2015_Performance-oriented DevOps.pdf;C\:\\Users\\master\\Zotero\\storage\\3DKEU57U\\1508.html}
}

@online{cabreraRealworldMachineLearning2023,
  title = {Real-World {{Machine Learning Systems}}: {{A}} Survey from a {{Data-Oriented Architecture Perspective}}},
  shorttitle = {Real-World {{Machine Learning Systems}}},
  author = {Cabrera, Christian and Paleyes, Andrei and Thodoroff, Pierre and Lawrence, Neil D.},
  date = {2023-10-09},
  eprint = {2302.04810},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.04810},
  url = {http://arxiv.org/abs/2302.04810},
  urldate = {2024-03-04},
  abstract = {Machine Learning models are being deployed as parts of real-world systems with the upsurge of interest in artificial intelligence. The design, implementation, and maintenance of such systems are challenged by real-world environments that produce larger amounts of heterogeneous data and users requiring increasingly faster responses with efficient resource consumption. These requirements push prevalent software architectures to the limit when deploying ML-based systems. Data-oriented Architecture (DOA) is an emerging concept that equips systems better for integrating ML models. DOA extends current architectures to create data-driven, loosely coupled, decentralised, open systems. Even though papers on deployed ML-based systems do not mention DOA, their authors made design decisions that implicitly follow DOA. The reasons why, how, and the extent to which DOA is adopted in these systems are unclear. Implicit design decisions limit the practitioners' knowledge of DOA to design ML-based systems in the real world. This paper answers these questions by surveying real-world deployments of ML-based systems. The survey shows the design decisions of the systems and the requirements these satisfy. Based on the survey findings, we also formulate practical advice to facilitate the deployment of ML-based systems. Finally, we outline open challenges to deploying DOA-based systems that integrate ML models.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\LW8UVSCZ\\Cabrera et al_2023_Real-world Machine Learning Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\QPIQYQVX\\2302.html}
}

@inproceedings{cardososilvaBenchmarkingMachineLearning2020,
  title = {Benchmarking {{Machine Learning Solutions}} in {{Production}}},
  booktitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Cardoso Silva, Lucas and Rezende Zagatti, Fernando and Silva Sette, Bruno and Nildaimon dos Santos Silva, Lucas and Lucrédio, Daniel and Furtado Silva, Diego and de Medeiros Caseli, Helena},
  options = {useprefix=true},
  date = {2020-12},
  pages = {626--633},
  doi = {10.1109/ICMLA51294.2020.00104},
  abstract = {Machine learning (ML) is becoming critical to many businesses. Keeping an ML solution online and responding is therefore a necessity, and is part of the MLOps (Machine Learning operationalization) movement. One aspect for this process is monitoring not only prediction quality, but also system resources. This is important to correctly provide the necessary infrastructure, either using a fully-managed cloud platform or a local solution. This is not a difficult task, as there are many tools available. However, it requires some planning and knowledge about what to monitor. Also, many ML professionals are not experts in system operations and may not have the skills to easily setup a monitoring and benchmarking environment. In the spirit of MLOps, this paper presents an approach, based on a simple API and set of tools, to monitor ML solutions. The approach was tested with 9 different solutions. The results indicate that the approach can deliver useful information to help in decision making, proper resource provision and operation of ML systems.},
  eventtitle = {2020 19th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  keywords = {Benchmark,Benchmark testing,Machine learning,Machine Learning,MLOps,Monitoring,Production,Systems operation,Task analysis,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\7WJUB6UG\\Cardoso Silva et al_2020_Benchmarking Machine Learning Solutions in Production.pdf;C\:\\Users\\master\\Zotero\\storage\\3PXPYX5A\\9356298.html}
}

@article{cawleyOverfittingModelSelection,
  title = {On {{Over-ﬁtting}}g in{{Model Selection}}n and{{Subsequent Selection Bias}}s in{{Performance Evaluation}}},
  author = {Cawley, Gavin C and Talbot, Nicola L C},
  abstract = {Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\9KBI9V8C\Cawley_Talbot_On Over-ﬁtting in Model Selection and Subsequent Selection Bias in Performance.pdf}
}

@article{chenDeepLearningEdge2019,
  title = {Deep {{Learning With Edge Computing}}: {{A Review}}},
  shorttitle = {Deep {{Learning With Edge Computing}}},
  author = {Chen, Jiasi and Ran, Xukan},
  date = {2019-08},
  journaltitle = {Proceedings of the IEEE},
  shortjournal = {Proc. IEEE},
  volume = {107},
  number = {8},
  pages = {1655--1674},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2019.2921977},
  url = {https://ieeexplore.ieee.org/document/8763885/},
  urldate = {2023-07-29},
  abstract = {Deep learning is currently widely used in a variety of applications, including computer vision and natural language processing. End devices such as smartphones and IoT sensors are generating data that need to be analyzed in real-time using deep learning or used to train deep learning models. However, deep learning inference and training require substantial computation resources to run quickly. Edge computing, where a fine mesh of compute nodes are placed close to end devices, is a viable way to meet the high computation and low latency requirements of deep learning on edge devices, and also provides additional benefits in terms of privacy, bandwidth efficiency, and scalability.},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\H3RYTRQL\Chen_Ran_2019_Deep Learning With Edge Computing.pdf}
}

@inproceedings{chenDevelopmentsMLflowSystem2020,
  title = {Developments in {{MLflow}}: {{A System}} to {{Accelerate}} the {{Machine Learning Lifecycle}}},
  shorttitle = {Developments in {{MLflow}}},
  booktitle = {Proceedings of the {{Fourth International Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  author = {Chen, Andrew and Chow, Andy and Davidson, Aaron and DCunha, Arjun and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Mewald, Clemens and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Singh, Avesh and Xie, Fen and Zaharia, Matei and Zang, Richard and Zheng, Juntai and Zumar, Corey},
  date = {2020-06-17},
  series = {{{DEEM}}'20},
  pages = {1--4},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3399579.3399867},
  url = {https://doi.org/10.1145/3399579.3399867},
  urldate = {2023-10-24},
  abstract = {MLflow is a popular open source platform for managing ML development, including experiment tracking, reproducibility, and deployment. In this paper, we discuss user feedback collected since MLflow was launched in 2018, as well as three major features we have introduced in response to this feedback: a Model Registry for collaborative model management and review, tools for simplifying ML code instrumentation, and experiment analytics functions for extracting insights from millions of ML experiments.},
  isbn = {978-1-4503-8023-2}
}

@online{coteQualityIssuesMachine2022,
  title = {Quality Issues in {{Machine Learning Software Systems}}},
  author = {Côté, Pierre-Olivier and Nikanjam, Amin and Bouchoucha, Rached and Khomh, Foutse},
  date = {2022-08-22},
  eprint = {2208.08982},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2208.08982},
  url = {http://arxiv.org/abs/2208.08982},
  urldate = {2022-12-07},
  abstract = {Context: An increasing demand is observed in various domains to employ Machine Learning (ML) for solving complex problems. ML models are implemented as software components and deployed in Machine Learning Software Systems (MLSSs). Problem: There is a strong need for ensuring the serving quality of MLSSs. False or poor decisions of such systems can lead to malfunction of other systems, significant financial losses, or even threat to human life. The quality assurance of MLSSs is considered as a challenging task and currently is a hot research topic. Moreover, it is important to cover all various aspects of the quality in MLSSs. Objective: This paper aims to investigate the characteristics of real quality issues in MLSSs from the viewpoint of practitioners. This empirical study aims to identify a catalog of bad-practices related to poor quality in MLSSs. Method: We plan to conduct a set of interviews with practitioners/experts, believing that interviews are the best method to retrieve their experience and practices when dealing with quality issues. We expect that the catalog of issues developed at this step will also help us later to identify the severity, root causes, and possible remedy for quality issues of MLSSs, allowing us to develop efficient quality assurance tools for ML models and MLSSs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\NPNEQEVA\\Côté et al_2022_Quality issues in Machine Learning Software Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\ES8GKMEI\\2208.html}
}

@online{dahlBenchmarkingNeuralNetwork2023,
  title = {Benchmarking {{Neural Network Training Algorithms}}},
  author = {Dahl, George E. and Schneider, Frank and Nado, Zachary and Agarwal, Naman and Sastry, Chandramouli Shama and Hennig, Philipp and Medapati, Sourabh and Eschenhagen, Runa and Kasimbeg, Priya and Suo, Daniel and Bae, Juhan and Gilmer, Justin and Peirson, Abel L. and Khan, Bilal and Anil, Rohan and Rabbat, Mike and Krishnan, Shankar and Snider, Daniel and Amid, Ehsan and Chen, Kongtao and Maddison, Chris J. and Vasudev, Rakshith and Badura, Michal and Garg, Ankush and Mattson, Peter},
  date = {2023-06-12},
  eprint = {2306.07179},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2306.07179},
  url = {http://arxiv.org/abs/2306.07179},
  urldate = {2023-11-05},
  abstract = {Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to address these challenges, we introduce a new, competitive, time-to-result benchmark using multiple workloads running on fixed hardware, the AlgoPerf: Training Algorithms benchmark. Our benchmark includes a set of workload variants that make it possible to detect benchmark submissions that are more robust to workload changes than current widely-used methods. Finally, we evaluate baseline submissions constructed using various optimizers that represent current practice, as well as other optimizers that have recently received attention in the literature. These baseline results collectively demonstrate the feasibility of our benchmark, show that non-trivial gaps between methods exist, and set a provisional state-of-the-art for future benchmark submissions to try and surpass.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\VJRJBDX4\\Dahl et al_2023_Benchmarking Neural Network Training Algorithms.pdf;C\:\\Users\\master\\Zotero\\storage\\CZ6H62F7\\2306.html}
}

@article{daiAddressingModernPractical2023,
  title = {Addressing Modern and Practical Challenges in Machine Learning: A Survey of Online Federated and Transfer Learning},
  shorttitle = {Addressing Modern and Practical Challenges in Machine Learning},
  author = {Dai, Shuang and Meng, Fanlin},
  date = {2023-05-01},
  journaltitle = {Applied Intelligence},
  shortjournal = {Appl Intell},
  volume = {53},
  number = {9},
  pages = {11045--11072},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-04065-3},
  url = {https://doi.org/10.1007/s10489-022-04065-3},
  urldate = {2024-03-07},
  abstract = {Online federated learning (OFL) and online transfer learning (OTL) are two collaborative paradigms for overcoming modern machine learning challenges such as data silos, streaming data, and data security. This survey explores OFL and OTL throughout their major evolutionary routes to enhance understanding of online federated and transfer learning. Practical aspects of popular datasets and cutting-edge applications for online federated and transfer learning are also highlighted in this work. Furthermore, this survey provides insight into potential future research areas and aims to serve as a resource for professionals developing online federated and transfer learning frameworks.},
  langid = {english},
  keywords = {Federated transfer learning,Online federated learning,Online learning,Online transfer learning,Privacy-preserving},
  file = {C:\Users\master\Zotero\storage\7N7BIBWT\Dai_Meng_2023_Addressing modern and practical challenges in machine learning.pdf}
}

@article{dengMNISTDatabaseHandwritten2012,
  title = {The {{MNIST Database}} of {{Handwritten Digit Images}} for {{Machine Learning Research}} [{{Best}} of the {{Web}}]},
  author = {Deng, Li},
  date = {2012-11},
  journaltitle = {IEEE Signal Processing Magazine},
  volume = {29},
  number = {6},
  pages = {141--142},
  issn = {1558-0792},
  doi = {10.1109/MSP.2012.2211477},
  abstract = {In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.},
  eventtitle = {{{IEEE Signal Processing Magazine}}},
  keywords = {Machine learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\YD2EFMVK\\Deng_2012_The MNIST Database of Handwritten Digit Images for Machine Learning Research.pdf;C\:\\Users\\master\\Zotero\\storage\\8XXBPIR7\\6296535.html}
}

@online{dodgeFineTuningPretrainedLanguage2020,
  title = {Fine-{{Tuning Pretrained Language Models}}: {{Weight Initializations}}, {{Data Orders}}, and {{Early Stopping}}},
  shorttitle = {Fine-{{Tuning Pretrained Language Models}}},
  author = {Dodge, Jesse and Ilharco, Gabriel and Schwartz, Roy and Farhadi, Ali and Hajishirzi, Hannaneh and Smith, Noah},
  date = {2020-02-14},
  eprint = {2002.06305},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.06305},
  url = {http://arxiv.org/abs/2002.06305},
  urldate = {2023-07-30},
  abstract = {Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\KZLYZF2C\\Dodge et al_2020_Fine-Tuning Pretrained Language Models.pdf;C\:\\Users\\master\\Zotero\\storage\\BNDUS4FB\\2002.html}
}

@article{domingosFewUsefulThings2012,
  title = {A Few Useful Things to Know about Machine Learning},
  author = {Domingos, Pedro},
  date = {2012-10-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {55},
  number = {10},
  pages = {78--87},
  issn = {0001-0782},
  doi = {10.1145/2347736.2347755},
  url = {https://doi.org/10.1145/2347736.2347755},
  urldate = {2023-03-14},
  abstract = {Tapping into the "folk knowledge" needed to advance machine learning applications.},
  file = {C:\Users\master\Zotero\storage\J2LSZY59\Domingos_2012_A few useful things to know about machine learning.pdf}
}

@inproceedings{eistySurveySoftwareMetric2018,
  title = {A {{Survey}} of {{Software Metric Use}} in {{Research Software Development}}},
  booktitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  author = {Eisty, Nasir U. and Thiruvathukal, George K. and Carver, Jeffrey C.},
  date = {2018-10},
  pages = {212--222},
  doi = {10.1109/eScience.2018.00036},
  abstract = {Background: Breakthroughs in research increasingly depend on complex software libraries, tools, and applications aimed at supporting specific science, engineering, business, or humanities disciplines. The complexity and criticality of this software motivate the need for ensuring quality and reliability. Software metrics are a key tool for assessing, measuring, and understanding software quality and reliability. Aims: The goal of this work is to better understand how research software developers use traditional software engineering concepts, like metrics, to support and evaluate both the software and the software development process. One key aspect of this goal is to identify how the set of metrics relevant to research software corresponds to the metrics commonly used in traditional software engineering. Method: We surveyed research software developers to gather information about their knowledge and use of code metrics and software process metrics. We also analyzed the influence of demographics (project size, development role, and development stage) on these metrics. Results: The survey results, from 129 respondents, indicate that respondents have a general knowledge of metrics. However, their knowledge of specific SE metrics is lacking, their use even more limited. The most used metrics relate to performance and testing. Even though code complexity often poses a significant challenge to research software development, respondents did not indicate much use of code metrics. Conclusions: Research software developers appear to be interested and see some value in software metrics but may be encountering roadblocks when trying to use them. Further study is needed to determine the extent to which these metrics could provide value in continuous process improvement.},
  eventtitle = {2018 {{IEEE}} 14th {{International Conference}} on E-{{Science}} (e-{{Science}})},
  keywords = {Complexity theory,Software,Software engineering,Software metrics,Survey Software Metrics Software Engineering Research Software,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\9BLC6WCD\\Eisty et al_2018_A Survey of Software Metric Use in Research Software Development.pdf;C\:\\Users\\master\\Zotero\\storage\\QQIXAFZM\\8588655.html}
}

@inproceedings{fayyadKnowledgeDiscoveryData1996,
  title = {Knowledge {{Discovery}} and {{Data Mining}}: {{Towards}} a {{Unifying Framework}}},
  booktitle = {{{KDD-96 Proceedings}}},
  author = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
  date = {1996},
  pages = {7},
  abstract = {This paper presents a first step towards a unifying frameworkfor KnowledgeDiscovery in Databases. We describe finks betweendata milfing, knowledgediscovery, and other related fields. Wethen define the KDDprocess and basic data mining algorithms, discuss application issues and conclude with an analysis of challengesfacing practitioners in the field.},
  eventtitle = {{{KDD-96}}},
  langid = {english},
  file = {C\:\\Users\\master\\Zotero\\storage\\6RPQKHQ5\\Fayyad et al_Knowledge Discovery and Data Mining_annotated.pdf;C\:\\Users\\master\\Zotero\\storage\\QAFSNS2S\\Fayyad et al_Knowledge Discovery and Data Mining.pdf}
}

@article{fernandez-lozanoMethodologyDesignExperiments2016,
  title = {A Methodology for the Design of Experiments in Computational Intelligence with Multiple Regression Models},
  author = {Fernandez-Lozano, Carlos and Gestal, Marcos and Munteanu, Cristian R. and Dorado, Julian and Pazos, Alejandro},
  date = {2016-12-01},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {4},
  eprint = {27920952},
  eprinttype = {pmid},
  pages = {e2721},
  issn = {2167-8359},
  doi = {10.7717/peerj.2721},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5136129/},
  urldate = {2023-02-15},
  abstract = {The design of experiments and the validation of the results achieved with them are vital in any research study. This paper focuses on the use of different Machine Learning approaches for regression tasks in the field of Computational Intelligence and especially on a correct comparison between the different results provided for different methods, as those techniques are complex systems that require further study to be fully understood. A methodology commonly accepted in Computational intelligence is implemented in an R package called RRegrs. This package includes ten simple and complex regression models to carry out predictive modeling using Machine Learning and well-known regression algorithms. The framework for experimental design presented herein is evaluated and validated against RRegrs. Our results are different for three out of five state-of-the-art simple datasets and it can be stated that the selection of the best model according to our proposal is statistically significant and relevant. It is of relevance to use a statistical approach to indicate whether the differences are statistically significant using this kind of algorithms. Furthermore, our results with three real complex datasets report different best models than with the previously published methodology. Our final goal is to provide a complete methodology for the use of different steps in order to compare the results obtained in Computational Intelligence problems, as well as from other fields, such as for bioinformatics, cheminformatics, etc., given that our proposal is open and~modifiable.},
  pmcid = {PMC5136129},
  file = {C:\Users\master\Zotero\storage\R4QQLYJX\Fernandez-Lozano et al_2016_A methodology for the design of experiments in computational intelligence with.pdf}
}

@article{finzerDataScienceEducation2013,
  title = {The {{Data Science Education Dilemma}}},
  author = {Finzer, William},
  date = {2013},
  journaltitle = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {2},
  doi = {10.5070/T572013891},
  url = {https://escholarship.org/uc/item/7gv0q9dc},
  urldate = {2023-01-17},
  abstract = {The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter-disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\YWX2L6LE\Finzer_2013_The Data Science Education Dilemma.pdf}
}

@inproceedings{fischerOpenMLCTR23CuratedTabular2023,
  title = {{{OpenML-CTR23}} – {{A}} Curated Tabular Regression Benchmarking Suite},
  author = {Fischer, Sebastian Felix and Feurer, Matthias and Bischl, Bernd},
  date = {2023-08-15},
  url = {https://openreview.net/forum?id=HebAOoMm94},
  urldate = {2023-11-16},
  eventtitle = {{{AutoML Conference}} 2023 ({{Workshop}})},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\5R69T4T2\Fischer et al_2023_OpenML-CTR23 – A curated tabular regression benchmarking suite.pdf}
}

@article{formanApplestoApplesCrossValidationStudies,
  title = {Apples-to-{{Apples}} in {{Cross-Validation Studies}}: {{Pitfalls}} in {{Classifier Performance Measurement}}},
  author = {Forman, George and Scholz, Martin},
  abstract = {Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\747PD6Q2\Forman_Scholz_Apples-to-Apples in Cross-Validation Studies.pdf}
}

@online{hestnessDeepLearningScaling2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  date = {2017-12-01},
  eprint = {1712.00409},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1712.00409},
  urldate = {2023-07-29},
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\master\Zotero\storage\Z3LL9RNB\Hestness et al_2017_Deep Learning Scaling is Predictable, Empirically.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1503.02531},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2023-02-03},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\K9JTAWDA\\Hinton et al_2015_Distilling the Knowledge in a Neural Network.pdf;C\:\\Users\\master\\Zotero\\storage\\JENB6RVC\\1503.html}
}

@online{hofferTrainLongerGeneralize2018,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  shorttitle = {Train Longer, Generalize Better},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  date = {2018-01-01},
  eprint = {1705.08741},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1705.08741},
  url = {http://arxiv.org/abs/1705.08741},
  urldate = {2023-08-02},
  abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomena. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\CT97GYUN\\Hoffer et al_2018_Train longer, generalize better.pdf;C\:\\Users\\master\\Zotero\\storage\\9Q9H65Q6\\1705.html}
}

@online{imbreaAutomatedMachineLearning2021,
  title = {Automated {{Machine Learning Techniques}} for {{Data Streams}}},
  author = {Imbrea, Alexandru-Ionut},
  date = {2021-06-14},
  eprint = {2106.07317},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.07317},
  url = {http://arxiv.org/abs/2106.07317},
  urldate = {2023-01-25},
  abstract = {Automated machine learning techniques benefited from tremendous research progress in recently. These developments and the continuous-growing demand for machine learning experts led to the development of numerous AutoML tools. However, these tools assume that the entire training dataset is available upfront and that the underlying distribution does not change over time. These assumptions do not hold in a data stream mining setting where an unbounded stream of data cannot be stored and is likely to manifest concept drift. Industry applications of machine learning on streaming data become more popular due to the increasing adoption of real-time streaming patterns in IoT, microservices architectures, web analytics, and other fields. The research summarized in this paper surveys the state-of-the-art open-source AutoML tools, applies them to data collected from streams, and measures how their performance changes over time. For comparative purposes, batch, batch incremental and instance incremental estimators are applied and compared. Moreover, a meta-learning technique for online algorithm selection based on meta-feature extraction is proposed and compared while model replacement and continual AutoML techniques are discussed. The results show that off-the-shelf AutoML tools can provide satisfactory results but in the presence of concept drift, detection or adaptation techniques have to be applied to maintain the predictive accuracy over time.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\PWERNVYQ\\Imbrea_2021_Automated Machine Learning Techniques for Data Streams.pdf;C\:\\Users\\master\\Zotero\\storage\\ER4UAF2R\\2106.html}
}

@inproceedings{jabbariWhatDevOpsSystematic2016,
  title = {What Is {{DevOps}}? {{A Systematic Mapping Study}} on {{Definitions}} and {{Practices}}},
  shorttitle = {What Is {{DevOps}}?},
  booktitle = {Proceedings of the {{Scientific Workshop Proceedings}} of {{XP2016}}},
  author = {Jabbari, Ramtin and bin Ali, Nauman and Petersen, Kai and Tanveer, Binish},
  options = {useprefix=true},
  date = {2016-05-24},
  series = {{{XP}} '16 {{Workshops}}},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/2962695.2962707},
  url = {https://doi.org/10.1145/2962695.2962707},
  urldate = {2024-03-07},
  abstract = {Context: DevOps, the combination of Development and Operations, is a new way of thinking in the software engineering domain that recently received much attention. Given that DevOps is a new term and novel concept recently introduced, no common understanding of what it entails has been achieved yet. Consequently, definitions of DevOps often only represent a part that is relevant to the concept. Objective:This study aims to characterize DevOps by exploring central components of DevOps definitions reported in the literature, specifying practices explicitly proposed for DevOps and investigating the similarities and differences between DevOps and other existing methods in software engineering. Method: A systematic mapping study was conducted that used six electronic databases: IEEE, ACM, Inspec, Scopus, Wiley Online Library and Web of Science. Result: 44 studies have been selected that report a definition of DevOps, 15 studies explicitly stating DevOps practices, and 15 studies stating how DevOps is related to other existing methods. Papers in some cases stated a combination of a definition, practices, and relations to other methods, the total number of primary studies was 49. Conclusion: We proposed a definition for DevOps which may overcome inconsistencies over the various existing definitions of individual research studies. In addition, the practices explicitly proposed for DevOps have been presented as well as the relation to other software development methods.},
  isbn = {978-1-4503-4134-9},
  keywords = {DevOps definition,DevOps practice,Software development method},
  file = {C:\Users\master\Zotero\storage\EVTXXG87\Jabbari et al_2016_What is DevOps.pdf}
}

@online{janochaLossFunctionsDeep2017,
  title = {On {{Loss Functions}} for {{Deep Neural Networks}} in {{Classification}}},
  author = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
  date = {2017-02-18},
  eprint = {1702.05659},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1702.05659},
  url = {http://arxiv.org/abs/1702.05659},
  urldate = {2023-08-27},
  abstract = {Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\8U86FCTW\\Janocha_Czarnecki_2017_On Loss Functions for Deep Neural Networks in Classification.pdf;C\:\\Users\\master\\Zotero\\storage\\3PW72J4V\\1702.html}
}

@article{karlMultiObjectiveHyperparameterOptimization2023,
  title = {Multi-{{Objective Hyperparameter Optimization}} in {{Machine Learning}} – {{An Overview}}},
  author = {Karl, Florian and Pielok, Tobias and Moosbauer, Julia and Pfisterer, Florian and Coors, Stefan and Binder, Martin and Schneider, Lennart and Thomas, Janek and Richter, Jakob and Lang, Michel and Garrido-Merchán, Eduardo C. and Branke, Juergen and Bischl, Bernd},
  date = {2023-09-05},
  journaltitle = {ACM Transactions on Evolutionary Learning and Optimization},
  shortjournal = {ACM Trans. Evol. Learn. Optim.},
  doi = {10.1145/3610536},
  url = {https://dl.acm.org/doi/10.1145/3610536},
  urldate = {2023-10-17},
  abstract = {Hyperparameter optimization constitutes a large part of typical modern machine learning workflows. This arises from the fact that machine learning methods and corresponding preprocessing steps often only yield optimal performance when hyperparameters are properly tuned. But in many applications, we are not only interested in optimizing ML pipelines solely for predictive accuracy; additional metrics or constraints must be considered when determining an optimal configuration, resulting in a multi-objective optimization problem. This is often neglected in practice, due to a lack of knowledge and readily available software implementations for multi-objective hyperparameter optimization. In this work, we introduce the reader to the basics of multi-objective hyperparameter optimization and motivate its usefulness in applied ML. Furthermore, we provide an extensive survey of existing optimization strategies, both from the domain of evolutionary algorithms and Bayesian optimization. We illustrate the utility of MOO in several specific ML applications, considering objectives such as operating conditions, prediction time, sparseness, fairness, interpretability and robustness.},
  keywords = {Bayesian Optimization,Multi-Objective Hyperparameter Optimization,Neural Architecture Search},
  annotation = {Just Accepted},
  file = {C:\Users\master\Zotero\storage\RAGPRQPN\Karl et al_2023_Multi-Objective Hyperparameter Optimization in Machine Learning – An Overview.pdf}
}

@book{kounevSystemsBenchmarkingScientists2020,
  title = {Systems {{Benchmarking}}: {{For Scientists}} and {{Engineers}}},
  shorttitle = {Systems {{Benchmarking}}},
  author = {Kounev, Samuel and Lange, Klaus-Dieter and Von Kistowski, Jóakim},
  date = {2020},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-030-41705-5},
  url = {https://link.springer.com/10.1007/978-3-030-41705-5},
  urldate = {2023-08-23},
  isbn = {978-3-030-41704-8 978-3-030-41705-5},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\4KUA98VS\Kounev et al_2020_Systems Benchmarking.pdf}
}

@article{kreuzbergerMachineLearningOperations2023,
  title = {Machine {{Learning Operations}} ({{MLOps}}): {{Overview}}, {{Definition}}, and {{Architecture}}},
  shorttitle = {Machine {{Learning Operations}} ({{MLOps}})},
  author = {Kreuzberger, Dominik and Kühl, Niklas and Hirschl, Sebastian},
  date = {2023},
  journaltitle = {IEEE Access},
  volume = {11},
  pages = {31866--31879},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3262138},
  abstract = {The final goal of all industrial machine learning (ML) projects is to develop ML products and rapidly bring them into production. However, it is highly challenging to automate and operationalize ML products and thus many ML endeavors fail to deliver on their expectations. The paradigm of Machine Learning Operations (MLOps) addresses this issue. MLOps includes several aspects, such as best practices, sets of concepts, and development culture. However, MLOps is still a vague term and its consequences for researchers and professionals are ambiguous. To address this gap, we conduct mixed-method research, including a literature review, a tool review, and expert interviews. As a result of these investigations, we contribute to the body of knowledge by providing an aggregated overview of the necessary principles, components, and roles, as well as the associated architecture and workflows. Furthermore, we provide a comprehensive definition of MLOps and highlight open challenges in the field. Finally, this work provides guidance for ML researchers and practitioners who want to automate and operate their ML products with a designated set of technologies.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Automation,Bibliographies,CI/CD,Codes,Collaboration,DevOps,Interviews,machine learning,Machine learning,MLOps,operations,Training,workflow orchestration},
  file = {C\:\\Users\\master\\Zotero\\storage\\EJZL4WQN\\Kreuzberger et al_2023_Machine Learning Operations (MLOps).pdf;C\:\\Users\\master\\Zotero\\storage\\QTB7H5IF\\10081336.html}
}

@inproceedings{kumarResourceefficientMachineLearning2017,
  title = {Resource-Efficient Machine Learning in 2 {{KB RAM}} for the Internet of Things},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}} - {{Volume}} 70},
  author = {Kumar, Ashish and Goyal, Saurabh and Varma, Manik},
  date = {2017-08-06},
  series = {{{ICML}}'17},
  pages = {1935--1944},
  publisher = {JMLR.org},
  location = {Sydney, NSW, Australia},
  abstract = {This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices - such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30\% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing's L3 ranker when the model size is restricted to 300 bytes. Bonsai's code can be downloaded from (BonsaiCode).},
  file = {C:\Users\master\Zotero\storage\DTSA6WCD\Kumar et al_2017_Resource-efficient machine learning in 2 KB RAM for the internet of things.pdf}
}

@online{leBuildingHighlevelFeatures2012,
  title = {Building High-Level Features Using Large Scale Unsupervised Learning},
  author = {Le, Quoc V. and Ranzato, Marc'Aurelio and Monga, Rajat and Devin, Matthieu and Chen, Kai and Corrado, Greg S. and Dean, Jeff and Ng, Andrew Y.},
  date = {2012-07-12},
  eprint = {1112.6209},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1112.6209},
  url = {http://arxiv.org/abs/1112.6209},
  urldate = {2023-02-03},
  abstract = {We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting with these learned features, we trained our network to obtain 15.8\% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70\% relative improvement over the previous state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\HA2RAJ2C\\Le et al_2012_Building high-level features using large scale unsupervised learning.pdf;C\:\\Users\\master\\Zotero\\storage\\7CYMMR8Y\\1112.html}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date = {2015-05},
  journaltitle = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  url = {https://www.nature.com/articles/nature14539},
  urldate = {2023-06-15},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  issue = {7553},
  langid = {english},
  keywords = {Computer science,Mathematics and computing},
  file = {C:\Users\master\Zotero\storage\Z4NV27YU\LeCun et al_2015_Deep learning.pdf}
}

@article{lecunGradientbasedLearningApplied1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  date = {1998-11},
  journaltitle = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  url = {https://ieeexplore.ieee.org/document/726791},
  urldate = {2023-11-06},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  eventtitle = {Proceedings of the {{IEEE}}},
  file = {C:\Users\master\Zotero\storage\VPXBQSVP\Lecun et al_1998_Gradient-based learning applied to document recognition.pdf}
}

@online{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  date = {2018-07-13},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1807.05118},
  url = {http://arxiv.org/abs/1807.05118},
  urldate = {2023-02-22},
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  pubstate = {preprint},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\CZZSUU97\\Liaw et al_2018_Tune.pdf;C\:\\Users\\master\\Zotero\\storage\\V45V3LPN\\1807.html}
}

@online{liHyperbandNovelBanditBased2018,
  title = {Hyperband: {{A Novel Bandit-Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2018-06-18},
  eprint = {1603.06560},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1603.06560},
  url = {http://arxiv.org/abs/1603.06560},
  urldate = {2023-08-03},
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\SN4XNI9W\\Li et al_2018_Hyperband.pdf;C\:\\Users\\master\\Zotero\\storage\\TMDUQ4T5\\1603.html}
}

@inproceedings{linMicrosoftCOCOCommon2014,
  title = {Microsoft {{COCO}}: {{Common Objects}} in {{Context}}},
  shorttitle = {Microsoft {{COCO}}},
  booktitle = {Computer {{Vision}} – {{ECCV}} 2014},
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollár, Piotr and Zitnick, C. Lawrence},
  editor = {Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne},
  date = {2014},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {740--755},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-10602-1_48},
  abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn = {978-3-319-10602-1},
  langid = {english},
  keywords = {Common Object,Object Category,Object Detection,Object Instance,Scene Understanding},
  file = {C:\Users\master\Zotero\storage\Z4Q52KNW\Lin et al_2014_Microsoft COCO.pdf}
}

@inproceedings{linROUGEPackageAutomatic2004,
  title = {{{ROUGE}}: {{A Package}} for {{Automatic Evaluation}} of {{Summaries}}},
  shorttitle = {{{ROUGE}}},
  booktitle = {Text {{Summarization Branches Out}}},
  author = {Lin, Chin-Yew},
  date = {2004-07},
  pages = {74--81},
  publisher = {Association for Computational Linguistics},
  location = {Barcelona, Spain},
  url = {https://aclanthology.org/W04-1013},
  urldate = {2023-08-27},
  file = {C:\Users\master\Zotero\storage\UBCXNB3P\Lin_2004_ROUGE.pdf}
}

@online{liRecentDevelopmentsRecommender2023,
  title = {Recent {{Developments}} in {{Recommender Systems}}: {{A Survey}}},
  shorttitle = {Recent {{Developments}} in {{Recommender Systems}}},
  author = {Li, Yang and Liu, Kangbo and Satapathy, Ranjan and Wang, Suhang and Cambria, Erik},
  date = {2023-06-22},
  eprint = {2306.12680},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.12680},
  url = {http://arxiv.org/abs/2306.12680},
  urldate = {2024-03-05},
  abstract = {In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\9AJG5TAV\\Li et al_2023_Recent Developments in Recommender Systems.pdf;C\:\\Users\\master\\Zotero\\storage\\7AWAHA5L\\2306.html}
}

@online{liSystemMassivelyParallel2020,
  title = {A {{System}} for {{Massively Parallel Hyperparameter Tuning}}},
  author = {Li, Liam and Jamieson, Kevin and Rostamizadeh, Afshin and Gonina, Ekaterina and Hardt, Moritz and Recht, Benjamin and Talwalkar, Ameet},
  date = {2020-03-15},
  eprint = {1810.05934},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.05934},
  url = {http://arxiv.org/abs/1810.05934},
  urldate = {2023-08-03},
  abstract = {Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\BZGNEU25\\Li et al_2020_A System for Massively Parallel Hyperparameter Tuning.pdf;C\:\\Users\\master\\Zotero\\storage\\H3WBUUSG\\1810.html}
}

@online{lonesHowAvoidMachine2022,
  title = {How to Avoid Machine Learning Pitfalls: A Guide for Academic Researchers},
  shorttitle = {How to Avoid Machine Learning Pitfalls},
  author = {Lones, Michael A.},
  date = {2022-09-06},
  eprint = {2108.02497},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.02497},
  url = {http://arxiv.org/abs/2108.02497},
  urldate = {2022-12-07},
  abstract = {This document gives a concise outline of some of the common mistakes that occur when using machine learning techniques, and what can be done to avoid them. It is intended primarily as a guide for research students, and focuses on issues that are of particular concern within academic research, such as the need to do rigorous comparisons and reach valid conclusions. It covers five stages of the machine learning process: what to do before model building, how to reliably build models, how to robustly evaluate models, how to compare models fairly, and how to report results.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\278JM9KT\\Lones_2022_How to avoid machine learning pitfalls.pdf;C\:\\Users\\master\\Zotero\\storage\\6RWMHTU8\\2108.html}
}

@online{luoNeuralArchitectureOptimization2019,
  title = {Neural {{Architecture Optimization}}},
  author = {Luo, Renqian and Tian, Fei and Qin, Tao and Chen, Enhong and Liu, Tie-Yan},
  date = {2019-09-04},
  eprint = {1808.07233},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1808.07233},
  url = {http://arxiv.org/abs/1808.07233},
  urldate = {2023-01-25},
  abstract = {Automatic neural architecture design has shown its potential in discovering powerful neural network architectures. Existing methods, no matter based on reinforcement learning or evolutionary algorithms (EA), conduct architecture search in a discrete space, which is highly inefficient. In this paper, we propose a simple and efficient method to automatic neural architecture design based on continuous optimization. We call this new approach neural architecture optimization (NAO). There are three key components in our proposed approach: (1) An encoder embeds/maps neural network architectures into a continuous space. (2) A predictor takes the continuous representation of a network as input and predicts its accuracy. (3) A decoder maps a continuous representation of a network back to its architecture. The performance predictor and the encoder enable us to perform gradient based optimization in the continuous space to find the embedding of a new architecture with potentially better accuracy. Such a better embedding is then decoded to a network by the decoder. Experiments show that the architecture discovered by our method is very competitive for image classification task on CIFAR-10 and language modeling task on PTB, outperforming or on par with the best results of previous architecture search methods with a significantly reduction of computational resources. Specifically we obtain 1.93\% test set error rate for CIFAR-10 image classification task and 56.0 test set perplexity of PTB language modeling task. Furthermore, combined with the recent proposed weight sharing mechanism, we discover powerful architecture on CIFAR-10 (with error rate 2.93\%) and on PTB (with test set perplexity 56.6), with very limited computational resources (less than 10 GPU hours) for both tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\6K6PGBVJ\\Luo et al_2019_Neural Architecture Optimization.pdf;C\:\\Users\\master\\Zotero\\storage\\KCLHHPPH\\1808.html}
}

@online{maclaurinGradientbasedHyperparameterOptimization2015,
  title = {Gradient-Based {{Hyperparameter Optimization}} through {{Reversible Learning}}},
  author = {Maclaurin, Dougal and Duvenaud, David and Adams, Ryan P.},
  date = {2015-04-02},
  eprint = {1502.03492},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1502.03492},
  url = {http://arxiv.org/abs/1502.03492},
  urldate = {2023-02-03},
  abstract = {Tuning hyperparameters of learning algorithms is hard because gradients are usually unavailable. We compute exact gradients of cross-validation performance with respect to all hyperparameters by chaining derivatives backwards through the entire training procedure. These gradients allow us to optimize thousands of hyperparameters, including step-size and momentum schedules, weight initialization distributions, richly parameterized regularization schemes, and neural network architectures. We compute hyperparameter gradients by exactly reversing the dynamics of stochastic gradient descent with momentum.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\IXIGPQTZ\\Maclaurin et al_2015_Gradient-based Hyperparameter Optimization through Reversible Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\RTHEZPGN\\1502.html}
}

@article{makridakisStatisticalMachineLearning2018,
  title = {Statistical and {{Machine Learning}} Forecasting Methods: {{Concerns}} and Ways Forward},
  shorttitle = {Statistical and {{Machine Learning}} Forecasting Methods},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  date = {2018-03-27},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  number = {3},
  pages = {e0194889},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194889},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0194889},
  urldate = {2023-08-27},
  abstract = {Machine Learning (ML) methods have been proposed in the academic literature as alternatives to statistical ones for time series forecasting. Yet, scant evidence is available about their relative performance in terms of accuracy and computational requirements. The purpose of this paper is to evaluate such performance across multiple forecasting horizons using a large subset of 1045 monthly time series used in the M3 Competition. After comparing the post-sample accuracy of popular ML methods with that of eight traditional statistical ones, we found that the former are dominated across both accuracy measures used and for all forecasting horizons examined. Moreover, we observed that their computational requirements are considerably greater than those of statistical methods. The paper discusses the results, explains why the accuracy of ML models is below that of statistical ones and proposes some possible ways forward. The empirical results found in our research stress the need for objective and unbiased ways to test the performance of forecasting methods that can be achieved through sizable and open competitions allowing meaningful comparisons and definite conclusions.},
  langid = {english},
  keywords = {Algorithms,Artificial intelligence,Computing methods,Forecasting,Neural networks,Preprocessing,Statistical methods,Support vector machines},
  file = {C:\Users\master\Zotero\storage\VQB8HMTP\Makridakis et al_2018_Statistical and Machine Learning forecasting methods.pdf}
}

@article{mishraDevOpsSoftwareQuality2020,
  title = {{{DevOps}} and Software Quality: {{A}} Systematic Mapping},
  shorttitle = {{{DevOps}} and Software Quality},
  author = {Mishra, Alok and Otaiwi, Ziadoon},
  date = {2020-11-01},
  journaltitle = {Computer Science Review},
  shortjournal = {Computer Science Review},
  volume = {38},
  pages = {100308},
  issn = {1574-0137},
  doi = {10.1016/j.cosrev.2020.100308},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013720304081},
  urldate = {2023-01-17},
  abstract = {Quality pressure is one of the factors affecting processes for software development in its various stages. DevOps is one of the proposed solutions to such pressure. The primary focus of DevOps is to increase the deployment speed, frequency and quality. DevOps is a mixture of different developments and operations to its multitudinous ramifications in software development industries, DevOps have attracted the interest of many researchers. There are considerable literature surveys on this critical innovation in software development, yet, little attention has been given to DevOps impact on software quality. This research is aimed at analyzing the implications of DevOps features on software quality. DevOps can also be referred to a change in organization cultures aimed at removal of gaps between the development and operations of an organization. The adoption of DevOps in an organization provides many benefits including quality but also brings challenges to an organization. This study presents systematic mapping of the impact of DevOps on software quality. The results of this study provide a better understanding of DevOps on software quality for both professionals and researchers working in this area. The study shows research was mainly focused in automation, culture, continuous delivery, fast feedback of DevOps. There is need of further research in many areas of DevOps (for instance: measurement, development of metrics of different stages to assess its performance, culture, practices toward ensuring quality assurance, and quality factors such as usability, efficiency, software maintainability and portability).},
  langid = {english},
  keywords = {Automation,Development,DevOps,Measurement,Operations,Software,Software quality,Systematic mapping},
  file = {C\:\\Users\\master\\Zotero\\storage\\FI2BPQCJ\\Mishra_Otaiwi_2020_DevOps and software quality.pdf;C\:\\Users\\master\\Zotero\\storage\\NTDWUG9S\\S1574013720304081.html}
}

@article{morales-hernandezSurveyMultiobjectiveHyperparameter2023,
  title = {A Survey on Multi-Objective Hyperparameter Optimization Algorithms for Machine Learning},
  author = {Morales-Hernández, Alejandro and Van Nieuwenhuyse, Inneke and Rojas Gonzalez, Sebastian},
  date = {2023-08-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {56},
  number = {8},
  pages = {8043--8093},
  issn = {1573-7462},
  doi = {10.1007/s10462-022-10359-2},
  url = {https://doi.org/10.1007/s10462-022-10359-2},
  urldate = {2023-10-17},
  abstract = {Hyperparameter optimization (HPO) is a necessary step to ensure the best possible performance of Machine Learning (ML) algorithms. Several methods have been developed to perform HPO; most of these are focused on optimizing one performance measure (usually an error-based measure), and the literature on such single-objective HPO problems is vast. Recently, though, algorithms have appeared that focus on optimizing multiple conflicting objectives simultaneously. This article presents a systematic survey of the literature published between 2014 and 2020 on multi-objective HPO algorithms, distinguishing between metaheuristic-based algorithms, metamodel-based algorithms and approaches using a mixture of both. We also discuss the quality metrics used to compare multi-objective HPO procedures and present future research directions.},
  langid = {english},
  keywords = {Hyperparameter optimization,Machine learning,Meta-heuristic,Metamodel,Multi-objective optimization},
  file = {C:\Users\master\Zotero\storage\HZ3JA67J\Morales-Hernández et al_2023_A survey on multi-objective hyperparameter optimization algorithms for machine.pdf}
}

@online{moreschiEndtoEndMLOpsTools2023,
  title = {Toward {{End-to-End MLOps Tools Map}}: {{A Preliminary Study}} Based on a {{Multivocal Literature Review}}},
  shorttitle = {Toward {{End-to-End MLOps Tools Map}}},
  author = {Moreschi, Sergio and Recupito, Gilberto and Lenarduzzi, Valentina and Palomba, Fabio and Hastbacka, David and Taibi, Davide},
  date = {2023-04-06},
  eprint = {2304.03254},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.03254},
  url = {http://arxiv.org/abs/2304.03254},
  urldate = {2024-03-24},
  abstract = {MLOps tools enable continuous development of machine learning, following the DevOps process. Different MLOps tools have been presented on the market, however, such a number of tools often create confusion on the most appropriate tool to be used in each DevOps phase. To overcome this issue, we conducted a multivocal literature review mapping 84 MLOps tools identified from 254 Primary Studies, on the DevOps phases, highlighting their purpose, and possible incompatibilities. The result of this work will be helpful to both practitioners and researchers, as a starting point for future investigations on MLOps tools, pipelines, and processes.},
  pubstate = {preprint},
  keywords = {Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\VK74VV5T\\Moreschi et al. - 2023 - Toward End-to-End MLOps Tools Map A Preliminary S.pdf;C\:\\Users\\master\\Zotero\\storage\\9KLYLFZW\\2304.html}
}

@article{myllyahoMisbehaviourFaultTolerance2022,
  title = {On Misbehaviour and Fault Tolerance in Machine Learning Systems},
  author = {Myllyaho, Lalli and Raatikainen, Mikko and Männistö, Tomi and Nurminen, Jukka K. and Mikkonen, Tommi},
  date = {2022-01-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {183},
  pages = {111096},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2021.111096},
  url = {https://www.sciencedirect.com/science/article/pii/S016412122100193X},
  urldate = {2023-02-03},
  abstract = {Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts.},
  langid = {english},
  keywords = {Case study,Fault tolerance,Machine learning,Software architecture,Software engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\PF8PE5AX\\Myllyaho et al_2022_On misbehaviour and fault tolerance in machine learning systems.pdf;C\:\\Users\\master\\Zotero\\storage\\IRCSGYBT\\S016412122100193X.html}
}

@online{neyshaburExploringGeneralizationDeep2017,
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  date = {2017-07-06},
  eprint = {1706.08947},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.08947},
  url = {http://arxiv.org/abs/1706.08947},
  urldate = {2023-11-01},
  abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  pubstate = {preprint},
  keywords = {_tablet,Computer Science - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\HJDW7CET\\Neyshabur et al_2017_Exploring Generalization in Deep Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\Z4A2F3R4\\1706.html}
}

@inproceedings{niculescu-mizilPredictingGoodProbabilities2005,
  title = {Predicting Good Probabilities with Supervised Learning},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning},
  author = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  date = {2005-08-07},
  series = {{{ICML}} '05},
  pages = {625--632},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/1102351.1102430},
  url = {https://doi.org/10.1145/1102351.1102430},
  urldate = {2023-08-26},
  abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  isbn = {978-1-59593-180-1}
}

@article{olsonPMLBLargeBenchmark2017,
  title = {{{PMLB}}: A Large Benchmark Suite for Machine Learning Evaluation and Comparison},
  shorttitle = {{{PMLB}}},
  author = {Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.},
  date = {2017-12-11},
  journaltitle = {BioData Mining},
  shortjournal = {BioData Mining},
  volume = {10},
  number = {1},
  pages = {36},
  issn = {1756-0381},
  doi = {10.1186/s13040-017-0154-4},
  url = {https://doi.org/10.1186/s13040-017-0154-4},
  urldate = {2023-02-22},
  abstract = {The selection, development, or comparison of machine learning methods in data mining can be a difficult task based on the target problem and goals of a particular study. Numerous publicly available real-world and simulated benchmark datasets have emerged from different sources, but their organization and adoption as standards have been inconsistent. As such, selecting and curating specific benchmarks remains an unnecessary burden on machine learning practitioners and data scientists.},
  keywords = {Benchmarking,Data repository,Machine learning,Model evaluation},
  file = {C\:\\Users\\master\\Zotero\\storage\\DQMHLZIA\\Olson et al_2017_PMLB.pdf;C\:\\Users\\master\\Zotero\\storage\\T9Y2UVYZ\\s13040-017-0154-4.html}
}

@online{openaiIntroducingChatGPT2022,
  title = {Introducing {{ChatGPT}}},
  author = {OpenAI},
  date = {2022-11-30},
  url = {https://openai.com/blog/chatgpt},
  urldate = {2023-07-24},
  abstract = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  langid = {american},
  file = {C:\Users\master\Zotero\storage\6X4N5BY7\chatgpt.html}
}

@inproceedings{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An Imperative Style}}, {{High-Performance Deep Learning Library}}},
  shorttitle = {{{PyTorch}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  date = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  url = {https://papers.nips.cc/paper_files/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
  urldate = {2024-03-05},
  abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.},
  file = {C:\Users\master\Zotero\storage\ZJNRHI6I\Paszke et al_2019_PyTorch.pdf}
}

@article{pedregosaScikitlearnMachineLearning2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  shorttitle = {Scikit-Learn},
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  date = {2011},
  journaltitle = {Journal of Machine Learning Research},
  volume = {12},
  number = {85},
  pages = {2825--2830},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v12/pedregosa11a.html},
  urldate = {2023-10-25},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  file = {C:\Users\master\Zotero\storage\S77C2H4M\Pedregosa et al_2011_Scikit-learn.pdf}
}

@inproceedings{pereraImproveSoftwareQuality2017,
  title = {Improve Software Quality through Practicing {{DevOps}}},
  booktitle = {2017 {{Seventeenth International Conference}} on {{Advances}} in {{ICT}} for {{Emerging Regions}} ({{ICTer}})},
  author = {Perera, Pulasthi and Silva, Roshali and Perera, Indika},
  date = {2017-09},
  pages = {1--6},
  issn = {2472-7598},
  doi = {10.1109/ICTER.2017.8257807},
  abstract = {DevOps is extended from certain agile practices with a mix of patterns intended to improve collaboration between development and operation teams. The main purpose of this paper is to conduct a study on how DevOps practice has impacted to software quality. The secondary objective is to find how to improve quality efficiently. A literature survey has carried out to explore about current DevOps practices in industry. According to the literature survey, the conceptual research model was developed and five hypotheses were derived. Research objectives were accomplished by testing hypotheses using Pearson correlation. A linear model is derived based on the linear regression analysis. An online questionnaire was used to collect quantitative data whereas interviews with experts on DevOps and Quality assurance have been used to identify how to improve the quality of software by practicing DevOps. Recommendations are given based on interview feedback, hypotheses testing with regression analysis. According to the quantitative study, researchers have identified that quality of the software gets improved when practice DevOps by following CAMS (Culture, Automation, Measurement, Sharing) framework. Automation is the most critical factor to improve the software quality. As per the results of multiple regression analysis, it has proved culture, automation, measurement and sharing are important factors to consider to improve quality of the software. In conclusion it can be recommended to use DevOps to achieve high quality software.},
  eventtitle = {2017 {{Seventeenth International Conference}} on {{Advances}} in {{ICT}} for {{Emerging Regions}} ({{ICTer}})},
  keywords = {Automation,CAMS Framework,Companies,DevOps,ISO 9126,Quality,Software measurement,Software quality,Testing},
  file = {C\:\\Users\\master\\Zotero\\storage\\D8E3C6NR\\Perera et al_2017_Improve software quality through practicing DevOps.pdf;C\:\\Users\\master\\Zotero\\storage\\URNWQ29A\\8257807.html}
}

@article{posoldovaMachineLearningPipelines2020,
  title = {Machine {{Learning Pipelines}}: {{From Research}} to {{Production}}},
  shorttitle = {Machine {{Learning Pipelines}}},
  author = {Posoldova, Alexandra},
  date = {2020-11},
  journaltitle = {IEEE Potentials},
  volume = {39},
  number = {6},
  pages = {38--42},
  issn = {1558-1772},
  doi = {10.1109/MPOT.2020.3016280},
  abstract = {Machine learning (ML) and artificial intelligence (AI) are getting lot of attention these days, and an increasing number of people are interested in becoming data scientists. Many imagine ML/AI as a black box that does its magic and is somehow able to make predictions. Well, the magic is that an ML algorithm is designed in such a way that it is able to find a pattern in data that have the correct answers and use that to predict the next answer. The learning task may vary from predicting the next word in a sentence, a sentiment, or what else would you like to buy based on what you already have in your basket.},
  eventtitle = {{{IEEE Potentials}}},
  keywords = {Artificial intelligence,Artificial neural networks,Containers,Data models,Pipelines,Task analysis,Tools},
  file = {C\:\\Users\\master\\Zotero\\storage\\DUHHCVJD\\Posoldova_2020_Machine Learning Pipelines.pdf;C\:\\Users\\master\\Zotero\\storage\\ZS2XA5P2\\9258455.html}
}

@inproceedings{prabhuComputationallyBudgetedContinual2023,
  title = {Computationally {{Budgeted Continual Learning}}: {{What Does Matter}}?},
  shorttitle = {Computationally {{Budgeted Continual Learning}}},
  author = {Prabhu, Ameya and Hammoud, Hasan Abed Al Kader and Dokania, Puneet and Torr, Philip H. S. and Lim, Ser-Nam and Ghanem, Bernard and Bibi, Adel},
  date = {2023-06-01},
  pages = {3698--3707},
  publisher = {IEEE Computer Society},
  doi = {10.1109/CVPR52729.2023.00360},
  url = {https://www.computer.org/csdl/proceedings-article/cvpr/2023/012900d698/1POTVoubGta},
  urldate = {2024-03-07},
  abstract = {Continual Learning (CL) aims to sequentially train models on streams of incoming data that vary in distribution by preserving previous knowledge while adapting to new data. Current CL literature focuses on restricted access to previously seen data, while imposing no constraints on the computational budget for training. This is unreasonable for applications in-the-wild, where systems are primarily constrained by computational and time budgets, not storage. We revisit this problem with a large-scale benchmark and analyze the performance of traditional CL approaches in a compute-constrained setting, where effective memory samples used in training can be implicitly restricted as a consequence of limited computation. We conduct experiments evaluating various CL sampling strategies, distillation losses, and partial fine-tuning on two large-scale datasets, namely ImageNet2K and Continual Google Landmarks V2 in data incremental, class incremental, and time incremental settings. Through extensive experiments amounting to a total of over 1500 GPU-hours, we find that, under compute-constrained setting, traditional CL approaches, with no exception, fail to outperform a simple minimal baseline that samples uniformly from memory. Our conclusions are consistent in a different number of stream time steps, e.g., 20 to 200, and under several computational budgets. This suggests that most existing CL methods are particularly too computationally expensive for realistic budgeted deployment. Code for this project is available at: https://github.com/drimpossible/BudgetCL.},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  langid = {english},
  file = {C\:\\Users\\master\\Zotero\\storage\\4768XJ68\\Prabhu et al_2023_Computationally Budgeted Continual Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\TE8KPBXI\\Prabhu et al. - 2023 - Computationally Budgeted Continual Learning What .pdf}
}

@article{precheltAutomaticEarlyStopping1998,
  title = {Automatic Early Stopping Using Cross Validation: Quantifying the Criteria},
  shorttitle = {Automatic Early Stopping Using Cross Validation},
  author = {Prechelt, Lutz},
  date = {1998-06-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {11},
  number = {4},
  pages = {761--767},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00010-0},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608098000100},
  urldate = {2023-08-02},
  abstract = {Cross validation can be used to detect when overfitting starts during supervised training of a neural network; training is then stopped before convergence to avoid the overfitting (`early stopping'). The exact criterion used for cross validation based early stopping, however, is chosen in an ad-hoc fashion by most researchers or training is stopped interactively. To aid a more well-founded selection of the stopping criterion, 14 different automatic stopping criteria from three classes were evaluated empirically for their efficiency and effectiveness in 12 different classification and approximation tasks using multi-layer perceptrons with RPROP training. The experiments show that, on average, slower stopping criteria allow for small improvements in generalization (in the order of 4\%), but cost about a factor of 4 longer in training time.},
  langid = {english},
  keywords = {Cross validation,Early stopping,Empirical study,Generalization,Overfitting,Supervised learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\SF72UPQ2\\Prechelt_1998_Automatic early stopping using cross validation.pdf;C\:\\Users\\master\\Zotero\\storage\\4DAK9L4Z\\S0893608098000100.html}
}

@inproceedings{rasleyDeepSpeedSystemOptimizations2020,
  title = {{{DeepSpeed}}: {{System Optimizations Enable Training Deep Learning Models}} with {{Over}} 100 {{Billion Parameters}}},
  shorttitle = {{{DeepSpeed}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  date = {2020-08-20},
  series = {{{KDD}} '20},
  pages = {3505--3506},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3394486.3406703},
  url = {https://dl.acm.org/doi/10.1145/3394486.3406703},
  urldate = {2024-03-04},
  abstract = {Explore new techniques in Microsoft's open source library called DeepSpeed, which advances large model training by improving scale, speed, cost, and usability, unlocking the ability to train 100-billion-parameter models. DeepSpeed is compatible with PyTorch. One piece of our library, called ZeRO, is a new parallelized optimizer that greatly reduces the resources needed for model and data parallelism while massively increasing the number of parameters that can be trained. Researchers have used these breakthroughs to create Turing Natural Language Generation (Turing-NLG), which at the time of its release was the largest publicly known language model at 17 billion parameters. In addition we will also go over our latest transformer kernel advancements that led the DeepSpeed team to achieve the world fastest BERT pretraining record. The Zero Redundancy Optimizer (ZeRO) is a novel memory optimization technology for large-scale distributed deep learning. ZeRO can train deep learning models with over 100 billion parameters on the current generation of GPU clusters at three to five times the throughput of the current best system. It also presents a clear path to training models with trillions of parameters, demonstrating an unprecedented leap in deep learning system technology. DeepSpeed brings state-of-the-art training techniques, such as ZeRO, optimized kernels, distributed training, mixed precision, and checkpointing, through lightweight APIs compatible with PyTorch. With just a few lines of code changes to your PyTorch model, you can leverage DeepSpeed to address underlying performance challenges and boost the speed and scale of your training.},
  isbn = {978-1-4503-7998-4},
  keywords = {distributed deep learning,machine learning},
  file = {C:\Users\master\Zotero\storage\N8WR32VM\Rasley et al_2020_DeepSpeed.pdf}
}

@online{rivolliCharacterizingClassificationDatasets2019,
  title = {Characterizing Classification Datasets: A Study of Meta-Features for Meta-Learning},
  shorttitle = {Characterizing Classification Datasets},
  author = {Rivolli, Adriano and Garcia, Luís P. F. and Soares, Carlos and Vanschoren, Joaquin and de Carvalho, André C. P. L. F.},
  options = {useprefix=true},
  date = {2019-08-26},
  eprint = {1808.10406},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1808.10406},
  url = {http://arxiv.org/abs/1808.10406},
  urldate = {2023-01-25},
  abstract = {Meta-learning is increasingly used to support the recommendation of machine learning algorithms and their configurations. Such recommendations are made based on meta-data, consisting of performance evaluations of algorithms on prior datasets, as well as characterizations of these datasets. These characterizations, also called meta-features, describe properties of the data which are predictive for the performance of machine learning algorithms trained on them. Unfortunately, despite being used in a large number of studies, meta-features are not uniformly described, organized and computed, making many empirical studies irreproducible and hard to compare. This paper aims to deal with this by systematizing and standardizing data characterization measures for classification datasets used in meta-learning. Moreover, it presents MFE, a new tool for extracting meta-features from datasets and identifying more subtle reproducibility issues in the literature, proposing guidelines for data characterization that strengthen reproducible empirical research in meta-learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\RBLAW9V5\\Rivolli et al_2019_Characterizing classification datasets.pdf;C\:\\Users\\master\\Zotero\\storage\\GZWP5W9J\\1808.html}
}

@software{rodolaGiampaoloPsutil2023,
  title = {Giampaolo/Psutil},
  author = {Rodola, Giampaolo},
  date = {2023-11-01T16:53:04Z},
  origdate = {2014-05-23T14:01:48Z},
  url = {https://github.com/giampaolo/psutil},
  urldate = {2023-11-02},
  abstract = {Cross-platform lib for process and system monitoring in Python},
  keywords = {cpu,disk,freebsd,linux,memory,monitoring,netbsd,openbsd,osx,ps,psutil,python,sensors,system-monitoring,top,windows}
}

@online{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2112.10752},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2023-07-22},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {C\:\\Users\\master\\Zotero\\storage\\I5XNBHGL\\Rombach et al_2022_High-Resolution Image Synthesis with Latent Diffusion Models.pdf;C\:\\Users\\master\\Zotero\\storage\\XS89TD75\\Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf;C\:\\Users\\master\\Zotero\\storage\\4Y2J95DE\\2112.html}
}

@article{sarkerMachineLearningAlgorithms2021,
  title = {Machine {{Learning}}: {{Algorithms}}, {{Real-World Applications}} and {{Research Directions}}},
  shorttitle = {Machine {{Learning}}},
  author = {Sarker, Iqbal H.},
  date = {2021-03-22},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {3},
  pages = {160},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00592-x},
  url = {https://doi.org/10.1007/s42979-021-00592-x},
  urldate = {2024-03-07},
  abstract = {In the current age of the Fourth Industrial Revolution (4IR or Industry 4.0), the digital world has a wealth of data, such as Internet of Things (IoT) data, cybersecurity data, mobile data, business data, social media data, health data, etc. To intelligently analyze these data and develop the corresponding smart and automated~applications, the knowledge of artificial intelligence (AI), particularly, machine learning (ML) is the key. Various types of machine learning algorithms such as supervised, unsupervised, semi-supervised, and reinforcement learning exist in the area. Besides, the deep learning, which is part of a broader family of machine learning methods, can intelligently analyze the data on a large scale. In this paper, we present a comprehensive view on these machine learning algorithms that can be applied to enhance the intelligence and the capabilities of an application. Thus, this study’s key contribution is explaining the principles of different machine learning techniques and their applicability in various real-world application domains, such as cybersecurity~systems, smart cities, healthcare, e-commerce, agriculture, and many more. We also highlight the challenges and potential research directions based on our study. Overall, this paper aims to serve as a reference point for both academia and industry professionals as well as for decision-makers~in various real-world situations and~application areas, particularly from the technical point of view.},
  langid = {english},
  keywords = {Artificial intelligence,Data science,Data-driven decision-making,Deep learning,Intelligent applications,Machine learning,Predictive analytics},
  file = {C:\Users\master\Zotero\storage\YI26PT48\Sarker_2021_Machine Learning.pdf}
}

@inproceedings{sculleyHiddenTechnicalDebt2015a,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
  urldate = {2023-08-13},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.},
  file = {C:\Users\master\Zotero\storage\XDLRWE7U\Sculley et al_2015_Hidden Technical Debt in Machine Learning Systems.pdf}
}

@online{sergeevHorovodFastEasy2018,
  title = {Horovod: Fast and Easy Distributed Deep Learning in {{TensorFlow}}},
  shorttitle = {Horovod},
  author = {Sergeev, Alexander and Del Balso, Mike},
  date = {2018-02-20},
  eprint = {1802.05799},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.05799},
  url = {http://arxiv.org/abs/1802.05799},
  urldate = {2024-03-05},
  abstract = {Training modern deep learning models requires large amounts of computation, often provided by GPUs. Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications. First, the training library must support inter-GPU communication. Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead. Second, the user must modify his or her training code to take advantage of inter-GPU communication. Depending on the training library's API, the modification required may be either significant or minimal. Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training. In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow. Horovod is available under the Apache 2.0 license at https://github.com/uber/horovod},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\67V2CMLK\\Sergeev_Del Balso_2018_Horovod.pdf;C\:\\Users\\master\\Zotero\\storage\\AY2C5UVU\\1802.html}
}

@article{shallueMeasuringEffectsData2019,
  title = {Measuring the {{Effects}} of {{Data Parallelism}} on {{Neural Network Training}}},
  author = {Shallue, Christopher J. and Lee, Jaehoon and Antognini, Joseph and Sohl-Dickstein, Jascha and Frostig, Roy and Dahl, George E.},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {112},
  pages = {1--49},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/18-789.html},
  urldate = {2023-09-28},
  abstract = {Recent hardware developments have dramatically increased the scale of data parallelism available for neural network training. Among the simplest ways to harness next-generation hardware is to increase the batch size in standard mini-batch neural network training algorithms. In this work, we aim to experimentally characterize the effects of increasing the batch size on training time, as measured by the number of steps necessary to reach a goal out-of-sample error. We study how this relationship varies with the training algorithm, model, and data set, and find extremely large variation between workloads. Along the way, we show that disagreements in the literature on how batch size affects model quality can largely be explained by differences in metaparameter tuning and compute budgets at different batch sizes. We find no evidence that larger batch sizes degrade out-of-sample performance. Finally, we discuss the implications of our results on efforts to train neural networks much faster in the future. Our experimental data is publicly available as a database of 71,638,836 loss measurements taken over the course of training for 168,160 individual models across 35 workloads.},
  file = {C:\Users\master\Zotero\storage\LZPDZCG8\Shallue et al_2019_Measuring the Effects of Data Parallelism on Neural Network Training.pdf}
}

@online{shankarOperationalizingMachineLearning2022,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  date = {2022-09-16},
  eprint = {2209.09125},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.09125},
  url = {http://arxiv.org/abs/2209.09125},
  urldate = {2022-12-07},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  pubstate = {preprint},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {C\:\\Users\\master\\Zotero\\storage\\K7KRWSL2\\Shankar et al_2022_Operationalizing Machine Learning.pdf;C\:\\Users\\master\\Zotero\\storage\\AWM9SK8B\\2209.html}
}

@article{shmueliExplainPredict2010a,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  date = {2010-08-01},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {25},
  number = {3},
  eprint = {1101.0891},
  eprinttype = {arxiv},
  eprintclass = {stat},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {http://arxiv.org/abs/1101.0891},
  urldate = {2023-03-14},
  abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\master\\Zotero\\storage\\AAY68PXV\\Shmueli_2010_To Explain or to Predict.pdf;C\:\\Users\\master\\Zotero\\storage\\DT7EAVTS\\1101.html}
}

@inproceedings{smedsDevOpsDefinitionPerceived2015,
  title = {{{DevOps}}: {{A Definition}} and {{Perceived Adoption Impediments}}},
  shorttitle = {{{DevOps}}},
  booktitle = {Agile {{Processes}} in {{Software Engineering}} and {{Extreme Programming}}},
  author = {Smeds, Jens and Nybom, Kristian and Porres, Ivan},
  editor = {Lassenius, Casper and Dingsøyr, Torgeir and Paasivaara, Maria},
  date = {2015},
  series = {Lecture {{Notes}} in {{Business Information Processing}}},
  pages = {166--177},
  publisher = {Springer International Publishing},
  location = {Cham},
  doi = {10.1007/978-3-319-18612-2_14},
  abstract = {As the interest in DevOps continues to grow, there is an increasing need for software organizations to understand how to adopt it successfully. This study has as objective to clarify the concept and provide insight into existing challenges of adopting DevOps. First, the existing literature is reviewed. A definition of DevOps is then formed based on the literature by breaking down the concept into its defining characteristics. We interview 13 subjects in a software company adopting DevOps and, finally, we present 11 impediments for the company’s DevOps adoption that were identified based on the interviews.},
  isbn = {978-3-319-18612-2},
  langid = {english},
  keywords = {Cloud Service,Continuous Delivery,Cultural Aspect,Service Failure,Software Process Improvement},
  file = {C:\Users\master\Zotero\storage\TGE2W5BX\Smeds et al_2015_DevOps.pdf}
}

@article{sokolovaSystematicAnalysisPerformance2009,
  title = {A Systematic Analysis of Performance Measures for Classification Tasks},
  author = {Sokolova, Marina and Lapalme, Guy},
  date = {2009-07-01},
  journaltitle = {Information Processing \& Management},
  shortjournal = {Information Processing \& Management},
  volume = {45},
  number = {4},
  pages = {427--437},
  issn = {0306-4573},
  doi = {10.1016/j.ipm.2009.03.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457309000259},
  urldate = {2023-08-27},
  abstract = {This paper presents a systematic analysis of twenty four performance measures used in the complete spectrum of Machine Learning classification tasks, i.e., binary, multi-class, multi-labelled, and hierarchical. For each classification task, the study relates a set of changes in a confusion matrix to specific characteristics of data. Then the analysis concentrates on the type of changes to a confusion matrix that do not change a measure, therefore, preserve a classifier’s evaluation (measure invariance). The result is the measure invariance taxonomy with respect to all relevant label distribution changes in a classification problem. This formal analysis is supported by examples of applications where invariance properties of measures lead to a more reliable evaluation of classifiers. Text classification supplements the discussion with several case studies.},
  keywords = {Machine Learning,Performance evaluation,Text classification},
  file = {C\:\\Users\\master\\Zotero\\storage\\SBUJN78Q\\Sokolova_Lapalme_2009_A systematic analysis of performance measures for classification tasks.pdf;C\:\\Users\\master\\Zotero\\storage\\QZW2NMJB\\S0306457309000259.html}
}

@online{stabilityaiStableDiffusionPublic2022,
  title = {Stable {{Diffusion Public Release}}},
  author = {{Stability AI}},
  date = {2022-08-22},
  url = {https://stability.ai/blog/stable-diffusion-public-release},
  urldate = {2023-07-24},
  abstract = {We are delighted to announce the public release of Stable Diffusion and the launch of DreamStudio Lite.},
  langid = {british},
  organization = {Stability AI},
  file = {C:\Users\master\Zotero\storage\MM67JQZT\stable-diffusion-public-release.html}
}

@article{strubellEnergyPolicyConsiderations2020,
  title = {Energy and {{Policy Considerations}} for {{Modern Deep Learning Research}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {09},
  pages = {13693--13696},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i09.7123},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/7123},
  urldate = {2023-07-29},
  abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
  issue = {09},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\GYKMMFTG\Strubell et al_2020_Energy and Policy Considerations for Modern Deep Learning Research.pdf}
}

@inproceedings{tamburriSustainableMLOpsTrends2020,
  title = {Sustainable {{MLOps}}: {{Trends}} and {{Challenges}}},
  shorttitle = {Sustainable {{MLOps}}},
  booktitle = {2020 22nd {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}} ({{SYNASC}})},
  author = {Tamburri, Damian A.},
  date = {2020-09},
  pages = {17--23},
  doi = {10.1109/SYNASC51798.2020.00015},
  url = {https://ieeexplore.ieee.org/abstract/document/9356947},
  urldate = {2024-03-24},
  abstract = {Even simply through a GoogleTrends search it becomes clear that Machine-Learning Operations-or MLOps, for short-are climbing in interest from both a scientific and practical perspective. On the one hand, software components and middleware are proliferating to support all manners of MLOps, from AutoML (i.e., software which enables developers with limited machine-learning expertise to train high-quality models specific to their domain or data) to feature-specific ML engineering, e.g., Explainability and Interpretability. On the other hand, the more these platforms penetrate the day-to-day activities of software operations, the more the risk for AI Software becoming unsustainable from a social, technical, or organisational perspective. This paper offers a concise definition of MLOps and AI Software Sustainability and outlines key challenges in its pursuit.},
  eventtitle = {2020 22nd {{International Symposium}} on {{Symbolic}} and {{Numeric Algorithms}} for {{Scientific Computing}} ({{SYNASC}})},
  keywords = {DataOps,Decision making,Machine learning,Machine-Learning Operations,Market research,Middleware,MLOps,Scientific computing,Software Sustainability,Software systems,Sustainable development},
  file = {C\:\\Users\\master\\Zotero\\storage\\QMU4NCYV\\Tamburri_2020_Sustainable MLOps.pdf;C\:\\Users\\master\\Zotero\\storage\\JLY5KP3C\\9356947.html}
}

@online{therayteamMemoryManagementRay,
  title = {Memory {{Management}} — {{Ray}} 2.7.1},
  author = {{The Ray Team,}},
  url = {https://docs.ray.io/en/latest/ray-core/scheduling/memory-management.html},
  urldate = {2023-11-01},
  organization = {Ray},
  file = {C:\Users\master\Zotero\storage\SM4ZQGAF\memory-management.html}
}

@online{thompsonComputationalLimitsDeep2022,
  title = {The {{Computational Limits}} of {{Deep Learning}}},
  author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
  date = {2022-07-27},
  eprint = {2007.05558},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2007.05558},
  urldate = {2023-07-29},
  abstract = {Deep learning’s recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image classification, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article catalogs the extent of this dependency, showing that progress across a wide variety of applications is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationallyefficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C:\Users\master\Zotero\storage\L9G7ZIJD\Thompson et al_2022_The Computational Limits of Deep Learning.pdf}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  date = {2011-06},
  pages = {1521--1528},
  publisher = {IEEE},
  location = {Colorado Springs, CO, USA},
  doi = {10.1109/CVPR.2011.5995347},
  url = {http://ieeexplore.ieee.org/document/5995347/},
  urldate = {2023-08-27},
  abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algorithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, reducing it to a single benchmark performance number. Indeed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study using a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset generalization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regarding this very important, but largely neglected issue.},
  eventtitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4577-0394-2},
  langid = {english},
  file = {C:\Users\master\Zotero\storage\MC89PBKP\Torralba_Efros_2011_Unbiased look at dataset bias.pdf}
}

@online{touvronLLaMAOpenEfficient2023,
  title = {{{LLaMA}}: {{Open}} and {{Efficient Foundation Language Models}}},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  date = {2023-02-27},
  eprint = {2302.13971},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.13971},
  url = {http://arxiv.org/abs/2302.13971},
  urldate = {2023-07-24},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\master\\Zotero\\storage\\7QL95LTK\\Touvron et al_2023_LLaMA.pdf;C\:\\Users\\master\\Zotero\\storage\\BUKAMHIP\\2302.html}
}

@article{vanschorenOpenMLNetworkedScience2014,
  title = {{{OpenML}}: Networked Science in Machine Learning},
  shorttitle = {{{OpenML}}},
  author = {Vanschoren, Joaquin and van Rijn, Jan N. and Bischl, Bernd and Torgo, Luis},
  options = {useprefix=true},
  date = {2014-06-16},
  journaltitle = {ACM SIGKDD Explorations Newsletter},
  shortjournal = {SIGKDD Explor. Newsl.},
  volume = {15},
  number = {2},
  pages = {49--60},
  issn = {1931-0145},
  doi = {10.1145/2641190.2641198},
  url = {https://doi.org/10.1145/2641190.2641198},
  urldate = {2023-11-16},
  abstract = {Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.},
  file = {C:\Users\master\Zotero\storage\VTRPUCCX\Vanschoren et al_2014_OpenML.pdf}
}

@article{wallerIncludingPerformanceBenchmarks2015,
  title = {Including {{Performance Benchmarks}} into {{Continuous Integration}} to {{Enable DevOps}}},
  author = {Waller, Jan and Ehmke, Nils C. and Hasselbring, Wilhelm},
  date = {2015-04-03},
  journaltitle = {ACM SIGSOFT Software Engineering Notes},
  shortjournal = {SIGSOFT Softw. Eng. Notes},
  volume = {40},
  number = {2},
  pages = {1--4},
  issn = {0163-5948},
  doi = {10.1145/2735399.2735416},
  url = {https://doi.org/10.1145/2735399.2735416},
  urldate = {2023-01-17},
  abstract = {The DevOps movement intends to improve communication, collaboration, and integration between software developers (Dev) and IT operations professionals (Ops). Automation of software quality assurance is key to DevOps success. We present how automated performance benchmarks may be included into continuous integration. As an example, we report on regression benchmarks for application monitoring frameworks and illustrate the inclusion of automated benchmarks into continuous integration setups.},
  keywords = {Jenkins,Kieker,MooBench},
  file = {C:\Users\master\Zotero\storage\L5MUF3HG\Waller et al_2015_Including Performance Benchmarks into Continuous Integration to Enable DevOps.pdf}
}

@inproceedings{wirthCrispdmStandardProcess2000,
  title = {Crisp-Dm: Towards a Standard Process Modell for Data Mining},
  shorttitle = {Crisp-Dm},
  author = {Wirth, R. and Hipp, Jochen},
  date = {2000},
  url = {https://www.semanticscholar.org/paper/Crisp-dm%3A-towards-a-standard-process-modell-for-Wirth-Hipp/48b9293cfd4297f855867ca278f7069abc6a9c24},
  urldate = {2023-06-15},
  abstract = {The CRISP-DM (CRoss Industry Standard Process for Data Mining) project proposed a comprehensive process model for carrying out data mining projects. The process model is independent of both the industry sector and the technology used. In this paper we argue in favor of a standard process model for data mining and report some experiences with the CRISP-DM process model in practice. We applied and tested the CRISP-DM methodology in a response modeling application project. The final goal of the project was to specify a process which can be reliably and efficiently repeated by different people and adapted to different situations. The initial projects were performed by experienced data mining people; future projects are to be performed by people with lower technical skills and with very little time to experiment with different approaches. It turned out, that the CRISP-DM methodology with its distinction of generic and specialized process models provides both the structure and the flexibility necessary to suit the needs of both groups. The generic CRISP-DM process model is useful for planning, communication within and outside the project team, and documentation. The generic check-lists are helpful even for experienced people. The generic process model provides an excellent foundation for developing a specialized process model which prescribes the steps to be taken in detail and which gives practical advice for all these steps.},
  file = {C:\Users\master\Zotero\storage\3NW8IB87\Wirth_Hipp_2000_Crisp-dm.pdf}
}

@inproceedings{xanthopoulosPuttingHumanBack2020,
  title = {Putting the {{Human Back}} in the {{AutoML Loop}}},
  author = {Xanthopoulos, Iordanis and Tsamardinos, I. and Christophides, V. and Simon, Eric and Salinger, Alejandro},
  date = {2020},
  url = {https://www.semanticscholar.org/paper/Putting-the-Human-Back-in-the-AutoML-Loop-Xanthopoulos-Tsamardinos/7293b51020d422ff14515abc7c91962713ea8391},
  urldate = {2023-01-25},
  abstract = {Automated Machine Learning (AutoML) is a rapidly rising subfield of Machine Learning. AutoML aims to fully automate the machine learning process end-to-end, democratizing Machine Learning to non-experts and drastically increasing the productivity of expert analysts. So far, most comparisons of AutoML systems focus on quantitative criteria such as predictive performance and execution time. In this paper, we examine AutoML services for predictive modeling tasks from a user’s perspective, going beyond predictive performance. We present a wide palette of criteria and dimensions on which to evaluate and compare these services as a user. This qualitative comparative methodology is applied on seven AutoML systems, namely Auger.AI, BigML, H2O’s Driverless AI, Darwin, Just Add Data Bio, RapidMiner, and Watson. The comparison indicates the strengths and weaknesses of each service, the needs that it covers, the segment of users that is most appropriate for, and the possibilities for improvements.},
  eventtitle = {{{EDBT}}/{{ICDT Workshops}}},
  file = {C:\Users\master\Zotero\storage\SA4IQ5AH\Xanthopoulos et al_2020_Putting the Human Back in the AutoML Loop.pdf}
}

@article{yangHyperparameterOptimizationMachine2020,
  title = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}: {{Theory}} and {{Practice}}},
  shorttitle = {On {{Hyperparameter Optimization}} of {{Machine Learning Algorithms}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2020-11},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {415},
  eprint = {2007.15745},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {295--316},
  issn = {09252312},
  doi = {10.1016/j.neucom.2020.07.061},
  url = {http://arxiv.org/abs/2007.15745},
  urldate = {2023-01-25},
  abstract = {Machine learning algorithms have been used widely in various applications and areas. To fit a machine learning model into different problems, its hyper-parameters must be tuned. Selecting the best hyper-parameter configuration for machine learning models has a direct impact on the model's performance. It often requires deep knowledge of machine learning algorithms and appropriate hyper-parameter optimization techniques. Although several automatic optimization techniques exist, they have different strengths and drawbacks when applied to different types of problems. In this paper, optimizing the hyper-parameters of common machine learning models is studied. We introduce several state-of-the-art optimization techniques and discuss how to apply them to machine learning algorithms. Many available libraries and frameworks developed for hyper-parameter optimization problems are provided, and some open challenges of hyper-parameter optimization research are also discussed in this paper. Moreover, experiments are conducted on benchmark datasets to compare the performance of different optimization methods and provide practical examples of hyper-parameter optimization. This survey paper will help industrial users, data analysts, and researchers to better develop machine learning models by identifying the proper hyper-parameter configurations effectively.},
  keywords = {68T01 90C31,C.2.0,Computer Science - Machine Learning,I.2.0,I.2.2,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\6TZA7J63\\Yang_Shami_2020_On Hyperparameter Optimization of Machine Learning Algorithms.pdf;C\:\\Users\\master\\Zotero\\storage\\CNXX8V4E\\2007.html}
}

@article{yangIoTDataAnalytics2022,
  title = {{{IoT Data Analytics}} in {{Dynamic Environments}}: {{From An Automated Machine Learning Perspective}}},
  shorttitle = {{{IoT Data Analytics}} in {{Dynamic Environments}}},
  author = {Yang, Li and Shami, Abdallah},
  date = {2022-11},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {116},
  eprint = {2209.08018},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  pages = {105366},
  issn = {09521976},
  doi = {10.1016/j.engappai.2022.105366},
  url = {http://arxiv.org/abs/2209.08018},
  urldate = {2023-01-25},
  abstract = {With the wide spread of sensors and smart devices in recent years, the data generation speed of the Internet of Things (IoT) systems has increased dramatically. In IoT systems, massive volumes of data must be processed, transformed, and analyzed on a frequent basis to enable various IoT services and functionalities. Machine Learning (ML) approaches have shown their capacity for IoT data analytics. However, applying ML models to IoT data analytics tasks still faces many difficulties and challenges, specifically, effective model selection, design/tuning, and updating, which have brought massive demand for experienced data scientists. Additionally, the dynamic nature of IoT data may introduce concept drift issues, causing model performance degradation. To reduce human efforts, Automated Machine Learning (AutoML) has become a popular field that aims to automatically select, construct, tune, and update machine learning models to achieve the best performance on specified tasks. In this paper, we conduct a review of existing methods in the model selection, tuning, and updating procedures in the area of AutoML in order to identify and summarize the optimal solutions for every step of applying ML algorithms to IoT data analytics. To justify our findings and help industrial users and researchers better implement AutoML approaches, a case study of applying AutoML to IoT anomaly detection problems is conducted in this work. Lastly, we discuss and classify the challenges and research directions for this domain.},
  keywords = {68T01 90C31,C.2.0,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Systems and Control,I.2.0,I.2.2},
  file = {C\:\\Users\\master\\Zotero\\storage\\MHGKKJW5\\Yang_Shami_2022_IoT Data Analytics in Dynamic Environments.pdf;C\:\\Users\\master\\Zotero\\storage\\AFUU3C7X\\2209.html}
}

@online{yuHyperParameterOptimizationReview2020,
  title = {Hyper-{{Parameter Optimization}}: {{A Review}} of {{Algorithms}} and {{Applications}}},
  shorttitle = {Hyper-{{Parameter Optimization}}},
  author = {Yu, Tong and Zhu, Hong},
  date = {2020-03-12},
  eprint = {2003.05689},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2003.05689},
  url = {http://arxiv.org/abs/2003.05689},
  urldate = {2024-01-10},
  abstract = {Since deep neural networks were developed, they have made huge contributions to everyday lives. Machine learning provides more rational advice than humans are capable of in almost every aspect of daily life. However, despite this achievement, the design and training of neural networks are still challenging and unpredictable procedures. To lower the technical thresholds for common users, automated hyper-parameter optimization (HPO) has become a popular topic in both academic and industrial areas. This paper provides a review of the most essential topics on HPO. The first section introduces the key hyper-parameters related to model training and structure, and discusses their importance and methods to define the value range. Then, the research focuses on major optimization algorithms and their applicability, covering their efficiency and accuracy especially for deep learning networks. This study next reviews major services and toolkits for HPO, comparing their support for state-of-the-art searching algorithms, feasibility with major deep learning frameworks, and extensibility for new modules designed by users. The paper concludes with problems that exist when HPO is applied to deep learning, a comparison between optimization algorithms, and prominent approaches for model evaluation with limited computational resources.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\master\\Zotero\\storage\\NN4TH478\\Yu_Zhu_2020_Hyper-Parameter Optimization.pdf;C\:\\Users\\master\\Zotero\\storage\\M9N3PXP9\\2003.html}
}

@article{zahariaAcceleratingMachineLearning2018,
  title = {Accelerating the {{Machine Learning Lifecycle}} with {{MLflow}}},
  author = {Zaharia, M. and Chen, A. and Davidson, A. and Ghodsi, A. and Hong, S. and Konwinski, A. and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and Xie, Fen and Zumar, Corey},
  date = {2018},
  journaltitle = {IEEE Data Eng. Bull.},
  url = {https://www.semanticscholar.org/paper/Accelerating-the-Machine-Learning-Lifecycle-with-Zaharia-Chen/b2e0b79e6f180af2e0e559f2b1faba66b2bd578a},
  urldate = {2023-03-14},
  abstract = {Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLflow, an open source platform we recently launched to streamline the machine learning lifecycle. MLflow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.},
  file = {C:\Users\master\Zotero\storage\6RA7FM6Z\Zaharia et al_2018_Accelerating the Machine Learning Lifecycle with MLflow.pdf}
}

@inproceedings{zhangUsingPipelinePerformance2023,
  title = {Using {{Pipeline Performance Prediction}} to {{Accelerate AutoML Systems}}},
  booktitle = {Proceedings of the {{Seventh Workshop}} on {{Data Management}} for {{End-to-End Machine Learning}}},
  author = {Zhang, Haoxiang and López, Roque and Santos, Aécio and Piazentin Ono, Jorge and Bessa, Aline and Freire, Juliana},
  date = {2023-06-18},
  series = {{{DEEM}} '23},
  pages = {1--11},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3595360.3595856},
  url = {https://dl.acm.org/doi/10.1145/3595360.3595856},
  urldate = {2024-01-10},
  abstract = {Automatic machine learning (AutoML) systems aim to automate the synthesis of machine learning (ML) pipelines. An important challenge these systems face is how to efficiently search a large space of candidate pipelines. Several strategies have been proposed to navigate and prune the search space, from the use of grammars to deep learning models. However, regardless of the strategy used, a major overhead lies in the evaluation step: for each synthesized pipeline p, these systems must both train and test p to guide the search and to identify the best pipelines. Given a time budget and computing resources, the evaluation cost limits how much of the search space can be explored. As a result, these systems may miss good pipelines. We propose ML4ML, an approach that aims to reduce the evaluation overhead for AutoML systems. ML4ML leverages the provenance of prior pipeline runs to predict performance without having to re-train and test the pipelines. We present results of an experimental evaluation which demonstrates that not only can ML4ML build a reliable predictive model with low mean absolute error, but the integration of this model with AutoML systems leads to substantial speedups, enabling the systems to explore a larger number of pipelines and primitive combinations and derive pipelines at a much lower cost.},
  isbn = {9798400702044},
  file = {C:\Users\master\Zotero\storage\WCIC9LRF\Zhang et al_2023_Using Pipeline Performance Prediction to Accelerate AutoML Systems.pdf}
}
